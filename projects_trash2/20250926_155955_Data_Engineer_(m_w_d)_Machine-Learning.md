---
collected_at: '2025-09-26T15:59:55.562680'
collection_channel: email
company: Randstadt Digital Germany AG
provider_id: freelancermap
provider_name: Freelancermap
reference_id: '2923242'
scraped_date: '2025-09-26T15:59:55.563674'
source_url: https://www.freelancermap.at/nproj/2923242.html
state: rejected
state_history:
- note: 'LLM evaluation: 40% fit score'
  state: rejected
  timestamp: '2025-09-26T16:02:09.363308'
title: Data Engineer (m/w/d) Machine-Learning
---



# Data Engineer (m/w/d) Machine-Learning
**URL:** [https://www.freelancermap.at/nproj/2923242.html](https://www.freelancermap.at/nproj/2923242.html)
## Details
- **Auslastung:** Bewerben
- **Ansprechpartner:** Jeannine Keith

## Schlagworte
N/A

## Beschreibung
F√ºr unseren Kunden suche ich einen **Data Engineer (m/w/d) Machine-Learning** f√ºr ein remote Projekt. 
 (DazO) - Phase 0. Ziel des Projektes ist die Absicherung der Verfahren im produktiven Umfeld durch die Abl√∂sung des AIC (Asset Intelligence Center) hin zu einer sicheren und stabilen IT-Plattform auf Basis von Microsoft Fabric, welche das Konzept der datenzentrierten Organisation unterst√ºtzt. Es handelt sich in dieser ersten Phase um ein Abl√∂seprojekt, welches gleichzeitig den technologischen Grundstein f√ºr eine langfristig moderne, agile und datenstarke Organisation bildet. Scope des Projekts ist u.a. die Migration der aktuell auf der Plattform AIC laufenden Use Cases auf die Microsoft Azure Plattform. Ziel ist hier die Data Management Plattform (DMP), die durch die DB Systel betrieben und bereitgestellt wird. Ausgangslage ist hierbei, dass diese Use Cases im Wesentlichen Datenstrecken mit mittlerer bis komplexer Datenverarbeitung sind, die sich √ºber mehrere Plattformen, wie eine IOT-Plattform, den zentralen DB Cargo Data Lake √ºber die AIC Plattform bis hin zur Auslieferung an andere Systeme bzw. Plattformen erstreckt. Sowohl der DB Cargo Data Lake als auch die AIC Plattform befinden sich in der AWS Cloud. Die Migration der Use Cases in die MS Azure Cloud f√ºhrt zu einer Multi-Cloud-Umgebung, da nahezu alle Datenstrecken sich zuk√ºnftig √ºber den DB Cargo Data Lake hin zu der neuen Plattform in der DMP erstrecken. Konkretes Teilprojektziel ist die Sicherstellung, alle relevanten Use Cases der AIC Plattform bis Ende Dezember 2025 zu migrieren, damit AIC zum 31.12.2025 abgeschaltet werden kann. Im Wesentlichen wird diese Migration derzeit mit internen Kr√§ften durchgef√ºhrt. Zur Sicherstellung der Einhaltung des Projektziels wird hierzu externe Unterst√ºtzung angefragt. 
 Aufgabenbeschreibung: - Analyse der zu migrierenden Datenprodukte bzw. Datenstrecken
- Programmierung von Datenstrecken bzw. Datenprodukte in Microsoft Azure Synapse/Data Factory (DMP Plattform) unter Einhaltung der Entwicklungsvorgaben, √ºber Daten-Zulieferplattformen wie z.B. DB Cargo Data Lake (AWS)
- Implementierung mittlerer bis komplexerer Logiken in Python und Integration in die Datenverarbeitungsprozesse
- Implementierung von Monitoring zur √úberwachung der ordnungsgem√§√üen Funktions- und Arbeitsweise der Datenprozesse
- Beheben von Fehlern
- Dokumentation der entwickelten Datenprodukte bzw. Datenstrecken
- Durchf√ºhrung von Tests
- Erstellen von deploymentf√§higen Lieferpaketen f√ºr die betreffenden Datenstrecken bzw. Datenprodukte
 **Anforderungen must have, sind zwingend erforderlich auch in Jahren sowie dem wording und m√ºssen in den Projekten aufgef√ºhrt sein!** - Praktische Erfahrung in der Entwicklung von Data Engineering Pipelines mit Azure Data Factory, Python, SQL und Spark; Mind. 3 Jahre praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Erfahrung in der Entwicklung von Machine-Learning Pipelines; Mind. 3 Jahre praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch mind. 2 Projektreferenzen nachweisbar
- Erfahrung in der Integration heterogener Datenquellen (z. B. ERP-, Logistik-, IoT- oder Web-Daten) in Cloud-basierte Data Engineering Pipelines; Mind. 3 Jahre praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Praktische Erfahrung in der Modellierung und Verarbeitung von Betriebs- oder Sensordaten; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Erfahrung in der Umsetzung von Datenqualit√§ts- oder Teststrategien f√ºr Data Engineering Pipelines (z. B. Validierungen, Monitoring, Data Assurance Services); Praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
 
 Soll Anforderungen: - Erfahrung in der Implementierung und Optimierung von ETL-/ELT-Prozessen f√ºr die Verarbeitung gro√üer Datenmengen im CloudUmfeld (z. B. Performance-Tuning, Parallelisierung, Scheduling von Pipelines); Praktische Erfahrung ist im Lebenslauf nachvollziehbar und in mind. 2 Projektreferenzen nachweisbar
- Erfahrung in der Umsetzung von Data Governance und Datenqualit√§t in Datenplattformen; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Projekterfahrung in agilen Teams (Scrum/SAFe) und Nutzung von Jira/Confluence; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
 
 Zeitraum - von: 10.10.2025 Zeitraum - bis: 31.12.2025 Profile bis 29.09. um 14Uhr Tagessatz allin: 600‚Ç¨ Projektsprache: deutsch Ich freue mich auf die Zusendung Ihres Profils, inkl. Verf√ºgbarkeit

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-26T16:02:09.360953

### Pre-Evaluation Phase
- **Score:** 18/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 18%. Found tags: ['aws', 'ai', 'ml', 'cloud', 'argo', 'monitoring', 'pipeline', 'python', 'git', 'sql', 'integration', 'agile', 'scrum', 'deployment', 'data', 'etl', 'performance', 'remote', 'freelance', 'projekt', 'jira', 'confluence', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 40/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements (from the offer):
  - Azure Data Factory / Azure Synapse experience for developing Data Engineering pipelines
  - Python, SQL and Spark (min. 3 years, must be evident in CV + projects)
  - Practical experience building Machine-Learning pipelines (min. 3 years + ‚â•2 project references)
  - Integration of heterogeneous sources (ERP, IoT, log/ web data) into cloud pipelines (‚â•3 years)
  - Modeling and processing of operational / sensor data (practical experience & project evidence)
  - Data quality / test strategies, monitoring for pipelines
  - ETL/ELT performance optimization, parallelization, scheduling (nice-to-have: ‚â•2 project refs)
  - Experience with data governance, agile teams and tools (Jira/Confluence)

- Match vs CV ‚Äì strengths:
  - Strong cloud architecture and migration experience (extensive AWS, multi-account, Landing Zone, Data Lake work).  This shows transferable cloud/data-platform skills and multi-cloud familiarity.
  - Solid Python experience across multiple projects and tooling; practical DevOps/automation skills (Terraform, Docker, Ansible basics, CI/CD, GitLab/Jenkins pipelines).
  - Experience with data lakes, large system migrations, SAP BW/4HANA architecture on AWS and integration projects ‚Äî relevant to complex data-streets and enterprise scenarios.
  - ML/AI knowledge and certifications (AWS AI Practitioner, work with generative models and LangChain in Project Bot; experience with AWS Bedrock, SageMaker listed in training).  Demonstrates conceptual ML/AI competence and applied experimentation.
  - Good experience with monitoring / operational best practices (built monitoring concepts, instrumentation, security-hardening and automated patching in past projects).
  - Familiarity with IoT/operational data at a conceptual level via AWS IoT services listed in training and multi-platform integrations in projects.

- Gaps / risks (must-have mismatches):
  - No explicit, documented hands-on experience with Microsoft Azure Synapse or Azure Data Factory in CV or project list ‚Äî the role explicitly requires this (must-have).
  - No clear evidence of Spark usage in production across ‚â•3 years or in project references; Spark is called out as required.
  - Machine-learning pipeline experience: while CV shows AI/ML training and a generative-AI project (Project Bot), there are not ‚â•2 production ML-pipeline project references showing 3+ years of practical ML pipeline development (training, serving, monitoring) as demanded.
  - Integration references exist (Oracle, SAP, Data Lake on AWS) but the offer requires 3+ years documented experience specifically integrating ERP/IoT/logistics data into cloud pipelines ‚Äî CV shows related work but not clearly mapped to the exact modern Azure pipeline context.
  - No explicit mention of Azure-specific data governance, Synapse performance tuning, or ADF deployment/monitoring experience.

- Conclusion / recommendation:
  - Andreas is a strong senior cloud/data architect with deep AWS, migration and automation experience, solid Python and operational skills, and practical exposure to AI tooling. These make him a good candidate for data-platform migration work in general and for multi-cloud projects.
  - However, the offer lists Azure Synapse / Data Factory and Spark experience as mandatory (‚â•3 years with project references). Those are not present in the CV as explicit deliverables, which is a major mismatch for a role that must be filled by someone who can immediately implement pipelines on the Microsoft DMP stack.
  - If the client accepts a short ramp-up and values AWS-to-Azure migration + strong engineering/architect skills, Andreas could likely be productive quickly. If the client requires strict compliance with the "must-have" Azure/ADF/Spark 3‚Äëyear evidence, this candidate will not meet that constraint.

- Fit-score justification (40/100):
  - +40 for strong transferable cloud, data-lake, Python, automation, ML conceptual knowledge and multi-cloud experience.
  - -60 for lack of mandatory, documented hands‚Äëon Azure Synapse / Data Factory and Spark experience and insufficient explicit production ML-pipeline references meeting the stated must-have criteria.

- Suggested next step: If you want to pursue him, ask Andreas to provide (a) concrete ADF/Synapse/Spark hands-on examples or proof of recent Azure work, or (b) a brief ramp-up plan showing how his AWS/data skills map to the MS DMP stack and an estimated time-to-productivity.

---
