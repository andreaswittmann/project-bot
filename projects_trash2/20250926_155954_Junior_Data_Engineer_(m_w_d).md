---
collected_at: '2025-09-26T15:59:54.869885'
collection_channel: email
company: Randstadt Digital Germany AG
provider_id: freelancermap
provider_name: Freelancermap
reference_id: '2923236'
scraped_date: '2025-09-26T15:59:54.870901'
source_url: https://www.freelancermap.at/nproj/2923236.html
state: rejected
state_history:
- note: 'LLM evaluation: 35% fit score'
  state: rejected
  timestamp: '2025-09-26T16:05:28.231110'
title: Junior Data Engineer (m/w/d)
---



# Junior Data Engineer (m/w/d)
**URL:** [https://www.freelancermap.at/nproj/2923236.html](https://www.freelancermap.at/nproj/2923236.html)
## Details
- **Auslastung:** Bewerben
- **Ansprechpartner:** Jeannine Keith

## Schlagworte
N/A

## Beschreibung
F√ºr unseren Kunden suche ich einen **Junior Data Engineer (m/w/d)** f√ºr ein remote Projekt. 
 Die vorliegende Anfrage ist Gegenstand des Projektvorhabens Datenzentrierte Organisation (DazO) - Phase 0. Ziel des Projektes ist die Absicherung der Verfahren im produktiven Umfeld durch die Abl√∂sung des AIC (Asset Intelligence Center) hin zu einer sicheren und stabilen IT-Plattform auf Basis von Microsoft Fabric, welche das Konzept der datenzentrierten Organisation unterst√ºtzt. Es handelt sich in dieser ersten Phase um ein Abl√∂seprojekt, welches gleichzeitig den technologischen Grundstein f√ºr eine langfristig moderne, agile und datenstarke Organisation bildet. 
 Scope des Projekts ist u.a. die Migration der aktuell auf der Plattform AIC laufenden Use Cases auf die Microsoft Azure Plattform. Ziel ist hier die Data Management Plattform (DMP), die durch die DB Systel betrieben und bereitgestellt wird. Ausgangslage ist hierbei, dass diese Use Cases im Wesentlichen Datenstrecken mit mittlerer bis komplexer Datenverarbeitung sind, die sich √ºber mehrere Plattformen, wie eine IOT-Plattform, den zentralen DB Cargo Data Lake √ºber die AIC Plattform bis hin zur Auslieferung an andere Systeme bzw. Plattformen erstreckt. Sowohl der DB Cargo Data Lake als auch die AIC Plattform befinden sich in der AWS Cloud. Die Migration der Use Cases in die MS Azure Cloud f√ºhrt zu einer Multi-Cloud-Umgebung, da nahezu alle Datenstrecken sich zuk√ºnftig √ºber den DB Cargo Data Lake hin zu der neuen Plattform in der DMP erstrecken. Konkretes Teilprojektziel ist die Sicherstellung, alle relevanten Use Cases der AIC Plattform bis Ende Dezember 2025 zu migrieren, damit AIC zum 31.12.2025 abgeschaltet werden kann. Im Wesentlichen wird diese Migration derzeit mit internen Kr√§ften durchgef√ºhrt. Zur Sicherstellung der Einhaltung des Projektziels wird hierzu externe Unterst√ºtzung angefragt. 
 Aufgabenbeschreibung: - Analyse der zu migrierenden Datenprodukte bzw. Datenstrecken
- Programmierung von Datenstrecken bzw. Datenprodukte in Microsoft Azure Synapse/Data Factory (DMP Plattform) unter Einhaltung der Entwicklungsvorgaben, √ºber Daten-Zulieferplattformen wie z.B. DB Cargo Data Lake (AWS)
- Implementierung mittlerer bis komplexerer Logiken in Python und Integration in die Datenverarbeitungsprozesse
- Implementierung von Monitoring zur √úberwachung der ordnungsgem√§√üen Funktions- und Arbeitsweise der Datenprozesse
- Beheben von Fehlern
- Dokumentation der entwickelten Datenprodukte bzw. Datenstrecken
- Durchf√ºhrung von Tests
- Erstellen von deploymentf√§higen Lieferpaketen f√ºr die betreffenden Datenstrecken bzw. Datenprodukte
 
 **Anforderungen must have, sind zwingend erforderlich auch in Jahren sowie dem wording und m√ºssen in den Projekten aufgef√ºhrt sein!** - Praktische Erfahrung in der Entwicklung von Data Engineering Pipelines mit Azure Data Factory, Azure Synapse, Python, SQL und Spark; Praktische Erfahrung in allen genannten Technologien ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Erfahrung in der Implementierung von ETL-/ELT-Prozessen f√ºr die Verarbeitung gro√üer Datenmengen im Cloud-Umfeld; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Erfahrung in der Integration heterogener Datenquellen (z. B. ERP-, Logistik-, IoT- oder Web-Daten) in Cloudbasierte Data Engineering Pipelines; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Erfahrung in der Implementierung und Durchf√ºhren von Tests zur Qualit√§tssicherung und √úberwachung von Datenstrecken in Azure; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Erfahrung in der Ad-Hoc Analyse von Daten mit Python, Pyspark und SQL; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und in Projektreferenzen nachweisbar
 
 Soll Anforderungen: - Erfahrung in der Entwicklung von Machine-Learning- oder Advanced-Analytics-Pipelines; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und in Projektreferenzen nachweisbar
- Projekterfahrung in agilen Teams (Scrum/SAFe) und Nutzung von Jira/Confluence; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und Projektreferenzen sind nachweisbar
- Erfahrung in der Entwicklung von Cloud Komponenten mit Azure; Durch Azure Fundamentals Zertifizierung nachweisbar
 
 Zeitraum - von: vsl.10.10.2025 Zeitraum - bis: 31.12.2025 Profile bis 29.09. um 14Uhr Tagessatz allin: 500‚Ç¨ Projektsprache: deutsch Ich freue mich auf die Zusendung Ihres Profils, inkl. Verf√ºgbarkeit

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-26T16:05:28.229275

### Pre-Evaluation Phase
- **Score:** 18/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 18%. Found tags: ['aws', 'ai', 'ml', 'cloud', 'argo', 'monitoring', 'pipeline', 'python', 'git', 'sql', 'integration', 'agile', 'scrum', 'deployment', 'data', 'analytics', 'etl', 'remote', 'freelance', 'projekt', 'jira', 'confluence', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 35/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key project must-haves: Azure Data Factory, Azure Synapse, Python, SQL, Spark (PySpark), ETL/ELT for large cloud data, integration of heterogeneous sources (IoT/ERP/logs), testing/monitoring of Azure data pipelines, ad‚Äëhoc analysis with Python/PySpark/SQL.  

- Azure Data Factory / Azure Synapse: Absent ‚Äî CV shows no explicit project experience or certification for ADF/Synapse or other Azure data platform services.  

- Spark / PySpark: Absent ‚Äî no explicit mention of Spark or PySpark in projects or skills.  

- Python & SQL: Present ‚Äî multiple projects list Python and extensive experience with Oracle/SQL databases and data migrations; useful for data engineering and ad‚Äëhoc analysis.  

- ETL/ELT & data pipelines (conceptual): Partial ‚Äî strong background in migration, data movement and integration (Oracle Forms/Reports, OSB, data lake migrations, AWS-based ETL-like work). However, experience is AWS/middleware-centric rather than Azure/ADF/Synapse-specific.  

- Integration of heterogeneous sources (IoT, logistics, ERP): Partial ‚Äî many integration and middleware projects (ESB, OSB, WebLogic) and experience with data-lake patterns on AWS; IoT is referenced in AWS training list but not as hands-on Azure/IOT-connector work.  

- Monitoring, testing, CI/CD: Present (but not Azure-specific) ‚Äî experience with automated tests, GitLab CI/CD, monitoring tools (CloudWatch, basic Prometheus/Grafana), and implementing monitoring/patching.  

- Cloud platform experience: Strong on AWS (multiple certifications and projects), Terraform, Kubernetes; however Azure-specific hands-on experience or certifications are missing.  

- Agile / tools: Present ‚Äî experience with Scrum/Kanban and Jira/Confluence references.  

Conclusion: Candidate brings strong cloud architecture, migration, Python and database skills and is well suited for cross-cloud/data migration patterns in general. The decisive gaps are lack of hands-on Azure Data Factory / Synapse and Spark/PySpark experience which are explicit must-haves for this role. With a short ramp-up or proof of quick Azure/ADF work, the candidate could be a viable mid/junior+ data engineer for the migration, but as-is this is a partial fit.

---
