---
collected_at: '2025-09-26T15:59:55.217855'
collection_channel: email
company: Randstadt Digital Germany AG
provider_id: freelancermap
provider_name: Freelancermap
reference_id: '2923245'
scraped_date: '2025-09-26T15:59:55.218959'
source_url: https://www.freelancermap.at/nproj/2923245.html
state: rejected
state_history:
- note: 'LLM evaluation: 60% fit score'
  state: rejected
  timestamp: '2025-09-26T16:00:31.492847'
title: Senior Data Architect (m/w/d)
---



# Senior Data Architect (m/w/d)
**URL:** [https://www.freelancermap.at/nproj/2923245.html](https://www.freelancermap.at/nproj/2923245.html)
## Details
- **Auslastung:** Bewerben
- **Ansprechpartner:** Jeannine Keith

## Schlagworte
N/A

## Beschreibung
Für unseren Kunden suche ich einen **Senior Data Architect** (m/w/d) für ein remote Projekt. 
 Die vorliegende Anfrage ist Gegenstand des Projektvorhabens Datenzentrierte Organisation (DazO) - Phase 0. Ziel des Projektes ist die Absicherung der Verfahren im produktiven Umfeld durch die Ablösung des AIC (Asset Intelligence Center) hin zu einer sicheren und stabilen IT-Plattform auf Basis von Microsoft Fabric, welche das Konzept der datenzentrierten Organisation unterstützt. Es handelt sich in dieser ersten Phase um ein Ablöseprojekt, welches gleichzeitig den technologischen Grundstein für eine langfristig moderne, agile und datenstarke Organisation bildet. 
 Scope des Projekts ist u.a. die Migration der aktuell auf der Plattform AIC laufenden Use Cases auf die Microsoft Azure Plattform. Ziel ist hier die Data Management Plattform (DMP), die durch die DB Systel betrieben und bereitgestellt wird. Ausgangslage ist hierbei, dass diese Use Cases im Wesentlichen Datenstrecken mit mittlerer bis komplexer Datenverarbeitung sind, die sich über mehrere Plattformen, wie eine IOT-Plattform, den zentralen DB Cargo Data Lake über die AIC Plattform bis hin zur Auslieferung an andere Systeme bzw. Plattformen erstreckt. Sowohl der DB Cargo Data Lake als auch die AIC Plattform befinden sich in der AWS Cloud. Die Migration der Use Cases in die MS Azure Cloud führt zu einer Multi-Cloud-Umgebung, da nahezu alle Datenstrecken sich zukünftig über den DB Cargo Data Lake hin zu der neuen Plattform in der DMP erstrecken. Konkretes Teilprojektziel ist die Sicherstellung, alle relevanten Use Cases der AIC Plattform bis Ende Dezember 2025 zu migrieren, damit AIC zum 31.12.2025 abgeschaltet werden kann. Im Wesentlichen wird diese Migration derzeit mit internen Kräften durchgeführt. Zur Sicherstellung der Einhaltung des Projektziels wird hierzu externe Unterstützung angefragt. 
 Aufgabenbeschreibung: - Analyse der zu migrierenden Datenprodukte bzw. Datenstrecken
- Programmierung von Datenstrecken bzw. Datenprodukte in Microsoft Azure Synapse/Data Factory (DMP Plattform) unter Einhaltung der Entwicklungsvorgaben, über Daten-Zulieferplattformen wie z.B. DB Cargo Data Lake (AWS)
- Implementierung mittlerer bis komplexerer Logiken in Python und Integration in die Datenverarbeitungsprozesse
- Implementierung von Monitoring zur Überwachung der ordnungsgemäßen Funktions- und Arbeitsweise der Datenprozesse
- Beheben von Fehlern
- Dokumentation der entwickelten Datenprodukte bzw. Datenstrecken
- Durchführung von Tests
- Erstellen von deploymentfähigen Lieferpaketen für die betreffenden Datenstrecken bzw. Datenprodukte
 
 **Anforderungen must have, sind zwingend erforderlich auch in Jahren sowie dem wording und müssen in den Projekten aufgeführt sein!** - Erfahrung im Bereich Data Architecture und Data Engineering in Cloud-Umgebungen; Mind. 5 Jahre praktische Erfahrung sind im Lebenslauf nachvollziehbar und durch mind. 5 Projektreferenzen nachweisbar
- Erfahrung in der Migration und Modernisierung von On-Premise-Systemen in CloudUmgebungen, inklusive Refactoring, Performance-Optimierung und Anpassung bestehender Datenstrecken an cloud-native Technologien; Mind. 2 Jahre praktische Erfahrung sind im Lebenslauf nachvollziehbar und mit mind. 3 Projektreferenzen nachweisbar
- Erfahrung in der Analyse & Aufbereitung von Daten im Kontext Schienengüterverkehr; Mind. 2 Jahre praktische Erfahrung sind im Lebenslauf nachvollziehbar und durch Projektreferenzen nachweisbar
- Planung, Design und Implementierung neuer Datenstrecken zur Aufbereitung und Verarbeitung transaktionaler Daten, inklusive, Modellierung von Datenflüssen sowie Entwicklung skalierbarer ETL-/ELT-Pipelines in SQL, Python, Spark, Nifi, AWS Glue und Dremio; 2 Jahre Praktische Erfahrung ist im Lebenslauf für alle Technologien nachvollziehbar und mit mind. 3 Projektreferenzen nachweisbar
 
 Anforderungen – should + nice to have: Umfangreiche Erfahrung in der Konzeption und Implementierung von CI/CD, Entwicklungsstandards und Teststrategien zur Sicherstellung von Datenqualität; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und mit mind. 3 Projektreferenzen nachweisbar Erfahrung in der Datenmodellierung komplexer Daten aus unterschiedlichen Quellsystemen (idealerweise einschließlich SAP); Praktische Erfahrung ist im Lebenslauf nachvollziehbar und mit mind. einer Projektreferenz nachweisbar Erfahrung in der Analyse und Aufbereitung von Transaktionalen Daten zur Near-Real-Time Analyse; Praktische Erfahrung ist im Lebenslauf nachvollziehbar und mit mind. einer Projektreferenz nachweisbar 
 Zeitraum - von: 10.10.2025 Zeitraum - bis: 31.12.2025 Profile bis 29.09. um 14Uhr Tagessatz allin: 600€ Projektsprache: deutsch Ich freue mich auf die Zusendung Ihres Profils, inkl. Verfügbarkeit

---

## 🤖 AI Evaluation Results

**Evaluation Timestamp:** 2025-09-26T16:00:31.491170

### Pre-Evaluation Phase
- **Score:** 20/100
- **Threshold:** 10/100
- **Result:** ✅ Passed
- **Rationale:** Score: 20%. Found tags: ['aws', 'ai', 'ml', 'cloud', 'argo', 'monitoring', 'architect', 'architecture', 'ci/cd', 'pipeline', 'python', 'git', 'sql', 'integration', 'agile', 'deployment', 'data', 'etl', 'performance', 'remote', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 60/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ❌ REJECTED

#### Detailed Rationale
- Matched / Strengths:
  - Extensive cloud architecture & migration experience (30 years IT, many AWS migrations and multi-account/landing-zone projects). Multiple AWS certifications (Solutions Architect Pro, SysOps, Developer) demonstrate deep cloud knowledge.
  - Proven track record with migrations, modernization and refactorings (many projects migrating on-prem to AWS, OSB/WebLogic migrations, Oracle Forms, enterprise integration). This addresses the general migration/modernization requirement and the need for production-readiness, CI/CD and operationalisation.
  - Strong ETL / data engineering skills: Python, SQL, EMR/AWS Glue (services listed in his AWS training), experience building data pipelines, monitoring, testing, documentation and deployment automation (IaC with Terraform, Docker, Git-based workflows).
  - Domain fit: multiple projects in the rail / schienengüterverkehr space (several logistics / rail clients), so the sector knowledge requirement is satisfied.
  - Operational monitoring and observability experience (Prometheus/Grafana familiarity, CloudWatch in AWS projects) and experience building deployment artefacts and runbooks.

- Gaps / Risks:
  - No explicit Microsoft Azure experience for the critical target technologies: Microsoft Fabric, Azure Synapse and Azure Data Factory are not mentioned in the CV or project list. The role specifically requires programming in Synapse/Data Factory and building on Microsoft Fabric.
  - Required technologies NiFi and Dremio are not referenced. Spark is plausible via EMR but not shown as hands-on Spark code in Synapse (Azure Databricks/Spark is not explicitly listed).
  - The job asks for explicit proof (years + specific project references) for several items (>=5 years Data Architecture with 5 references; 2 years migration to cloud with 3 references; etc.). While the CV shows many relevant migration/architecture projects, it does not provide explicit project entries that name Azure Synapse/Data Factory/Microsoft Fabric or Dremio/NiFi to match the wording/technology-level evidence demanded.
  - Migration target is AWS AIC -> Azure (multi-cloud). CV shows AWS-centric migrations and some hybrid/cloud-agnostic projects, but no clear prior AWS->Azure migrations or Microsoft Fabric experience.

- Practical assessment / recommendation:
  - Candidate has strong baseline to succeed quickly (deep cloud architecture, migration and pipeline experience, Python/SQL, data-lake experience, rail domain). With short ramp-up or a demonstrable small proof-of-concept on Azure Synapse / Data Factory (1–2 weeks), he could cover the missing technology gaps.
  - Because the role has a short, fixed deadline (migrations complete by 31.12.2025) and requests explicit technology evidence, the absence of documented Synapse/Data Factory / Microsoft Fabric experience reduces immediate fit for a plug-and-play assignment.

- Score justification (60/100):
  - +40 points: strong cloud architecture, migration experience, data pipeline & operational skills, rail-domain fit, and relevant certifications.
  - -40 points: missing explicit Microsoft Fabric / Azure Synapse / Data Factory experience, missing NiFi/Dremio references, and lack of the exact project-reference wording the client demands.
  - Net: 60/100 (good senior-level fit in architecture and data engineering; moderate risk for immediate hands-on Microsoft-Fabric/Synapse delivery without a short ramp-up).

---
