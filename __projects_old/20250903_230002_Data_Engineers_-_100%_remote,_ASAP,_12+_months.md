---
company: wynwood tech solutions GmbH
reference_id: '2915587'
scraped_date: '2025-09-03T23:00:02.668640'
source_url: https://www.freelancermap.de/projekt/data-engineers-100-prozent-remote-asap-12-months?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 68% fit score'
  state: rejected
  timestamp: '2025-09-03T23:01:09.171561'
title: Data Engineers - 100% remote, ASAP, 12+ months
---


# Data Engineers - 100% remote, ASAP, 12+ months
**URL:** [https://www.freelancermap.de/projekt/data-engineers-100-prozent-remote-asap-12-months?ref=rss](https://www.freelancermap.de/projekt/data-engineers-100-prozent-remote-asap-12-months?ref=rss)
## Details
- **Start:** ab sofort
- **Von:** wynwood tech solutions GmbH
- **Auslastung:** 100% (5 Tage pro Woche)
- **Eingestellt:** 03.09.2025
- **Ansprechpartner:** wynwood tech Recruiting Team
- **Projekt-ID:** 2915587
- **Branche:** IT
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 100
                                                % Remote

## Schlagworte
Information Engineering, Airflow, Amazon S3, Microsoft Azure, Datenbanken, Data Architecture, Datensicherheit, Distributed Computing, Python, Management Systeme, SQL, Google Cloud, Data Analysis, Cloud Computing, Vertragsmanagement, Workflows, Testen, Datenschutz, Freelancing

## Beschreibung
Dear Data Engineers,

We are currently looking for multiple Data Experts for one of the leading tech companies in Germany. The position is 100% remote and long-term.

QUICK FACTS:
- Freelancing/Contracting
- Start: ASAP
- 100% remote
- Duration: long-term 12+ months
- Capacity: full-time, part-time also possible
- Language: English (NO GERMAN NEEDED)
- German business hours

REQUIREMENTS:
- Experience in data engineering, including data architecture, analytics, and deployment pipelines
- Proficiency in Python and SQL for data processing, querying, and transformation
- Familiarity with distributed computing frameworks and database management systems
- Knowledge of cloud-based storage solutions such as Amazon S3, Google Cloud Storage, or Azure Blob Storage
- Experience with data testing practices, workflow scheduling tools (e.g., Apache Airflow, Prefect, Dagster), and understanding of data security and privacy patterns

Are you interested?

If so, please send us your most recent CV as well as your earliest possible starting date.
When sending over your details, please include a self-assessment (0-10, 10 = expert level) in the following areas:
- Data Engineering (in general)
- Python
- Distributed computing frameworks (please list the ones you are familiar with and rate yourself)
- SQL
- Database Management Systems (please list the ones you are familiar with and rate yourself)
- Cloud-based storage solutions (Amazon S3, Google Cloud Storage or Azure Blob Storage --> please list the ones you are familiar with and rate yourself)
- Data architecture
- Data analytics
- Data deployment pipelines
- Testing (Design and implementation)
- Data security and privacy patterns
- Workflow scheduling tools (Apache Airflow, Prefect, Dagster --> please list the ones you are familiar with and rate yourself)

We are looking forward to hearing back from you. Have a great day!

Your wynwood tech Recruiting Team ;-)

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-03T23:01:09.168111

### Pre-Evaluation Phase
- **Score:** 17/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 17%. Found tags: ['amazon', 'ai', 'ki', 'cloud', 'architect', 'architecture', 'pipeline', 'python', 'rest', 'database', 'sql', 'security', 'deployment', 'data', 'analytics', 'remote', 'freelance', 'projekt']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 68/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements from the offer:
  - Data engineering (architecture, analytics, deployment pipelines)
  - Python and SQL proficiency
  - Distributed computing frameworks (e.g. Spark, Hadoop, Flink, Kafka)
  - Database Management Systems
  - Cloud storage (Amazon S3, GCS, Azure Blob)
  - Workflow schedulers (Apache Airflow, Prefect, Dagster)
  - Data testing practices and data security/privacy patterns

- Match vs CV (Andreas Wittmann):
  - Data engineering / architecture / deployment pipelines: Strong. Long career as architect/consultant, many projects designing high-availability architectures, AWS Landing Zone, SAP BW/4HANA on AWS, migrations and IaC (Terraform). (High match)
  - Python: Present and used repeatedly for automation, migration scripts and tooling. Likely solid for engineering tasks (Good match)
  - SQL / DBMS: Strong. Extensive hands-on with Oracle (19c, 11g), RDS and many AWS data services listed in his AWS study material. (High match)
  - Cloud storage: Strong for AWS (S3). No explicit hands-on GCS or Azure Blob in projects ‚Äî AWS-focused. (Partial match)
  - Distributed computing frameworks: Limited explicit evidence of Spark/Hadoop/Flink. He has experience with messaging, clustering, and AWS streaming/EMR listed in studied services, and Kafka/MSK appears in AWS list, but no clear hands-on Spark/Big Data job. (Weak to partial match)
  - Workflow scheduling tools: No explicit Airflow/Prefect/Dagster experience listed. Shows CI/CD, Jenkins, GitLab CI/CD, Argo CD and Kubernetes experience ‚Äî transferable but not the toolset requested. (Partial/weak match)
  - Data testing practices: Has experience building automated functional/connection tests and CI pipelines ‚Äî relevant, but not explicit "data testing" (e.g., data quality frameworks). (Partial match)
  - Data security & privacy: Strong. Many projects with SSL, SAML, WS-Security, IAM, KMS and AWS security tooling; good understanding of security patterns. (High match)

- Overall assessment / recommendation:
  - Andreas is a senior cloud/architecture/DevOps-savvy candidate with strong AWS, SQL/Oracle, Python scripting and security skills ‚Äî a good fit where data platform architecture, reliability, security and deployment automation are key.
  - He lacks explicit evidence of core data-engineering tool experience requested by the client (Airflow/Prefect/Dagster, and Big Data frameworks such as Spark/Hadoop). If the client can accept strong cloud/data-infrastructure experience and willingness to ramp up specific orchestration / big-data frameworks quickly, Andreas is a solid candidate.

- Actionable next steps if you want to present him:
  - Ask Andreas to explicitly state any hands-on experience with Airflow / Prefect / Dagster and Spark/Hadoop, and provide examples or small code snippets / repos for data pipelines.
  - Confirm his experience with GCP/Azure storage if needed, or clarify willingness to work exclusively on AWS (where he is strong).

---
