---
company: Hays AG
reference_id: '2914560'
scraped_date: '2025-09-02T23:00:18.784731'
source_url: https://www.freelancermap.ch/projekt/natural-language-processing-expert-m-f-d-2914560?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 55% fit score'
  state: rejected
  timestamp: '2025-09-02T23:04:36.859165'
title: Natural language processing expert (m/f/d)
---


# Natural language processing expert (m/f/d)
**URL:** [https://www.freelancermap.ch/projekt/natural-language-processing-expert-m-f-d-2914560?ref=rss](https://www.freelancermap.ch/projekt/natural-language-processing-expert-m-f-d-2914560?ref=rss)
## Details
- **Start:** ab sofort
- **Von:** Hays AG
- **Eingestellt:** 01.09.2025
- **Ansprechpartner:** Hays AG
- **Projekt-ID:** 2914560
- **Branche:** IT
- **Vertragsart:** Freiberuflich

## Schlagworte
Natural Language Processing, K√ºnstliche Intelligenz, Architektur, Python, Machine Learning, Software Architecture, Transformer, Softwareentwicklung, Pytorch, Ready Api, Restful Apis, Docker

## Beschreibung
**Natural language processing expert (m/f/d)**

Reference: -en
Start: asap
Duration: 13 MM+

**Main tasks:**

- Review and optimize a transformer architecture and training pipeline
- Implement and experiment with advanced techniques to improve performance on fields with severe class imbalance
- Conduct in-depth error analysis to identify patterns in misclassifications and propose data-driven improvements
- Refine and validate the data processing, label mapping, and stratified data splitting procedures to ensure maximum reliability
- Collaborate with our software architect to integrate the final, optimized model into a production-ready API for inference
- Document the final model architecture, training procedures, performance benchmarks, and best practices for future development

**Main qualifications:**

- Educational background: advanced degree in AI/NLP or related field
- Hands-on experience fine-tuning Transformer models
- Demonstrable experience with multi-task and multi-label classification problems, expertise in handling severe class imbalance in text data
- Proficiency in deploying machine learning models as REST APIs
- Strong proficiency in PyTorch, including creating custom model architectures (e.g., multi-head classifiers) and custom loss functions
- Strong software engineering fundamentals and the ability to write clean, modular, and well-documented Python code, experience with Docker

**Main advantages:**

- Internal career opportunities
- Wolrd-renowned pharmaceutical company
- International and diverse environment

**About us:**
IT is and always has been our core business that laid the foundation for Hays' success. We are the biggest privately owned IT recruitment agency in Germany and offer the best jobs for every career level ‚Äì whether you are interested in vacancies in agile SMEs or international DAX groups. Hays masters the entire IT job spectrum, from support to software architecture or digitalisation ‚Äì thanks to our broad portfolio, we have something for everyone. In the last decades, we were able to support numerous IT experts with choosing the right path for a successful career, positioning ourselves as their lifelong partner. Our highly specialised consultants can cater to your every wish and expectation and will prepare you for interviews and contract negotiations. Give it a try and learn what the market has to offer ‚Äì our services are free of charge, non-binding and discreet! We look forward to hearing from you.

**My contact at Hays:**

Referencenumber:

Make contact:
Email:
Telefone:

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-02T23:04:36.855308

### Pre-Evaluation Phase
- **Score:** 18/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 18%. Found tags: ['ai', 'machine learning', 'docker', 'architect', 'architekt', 'architecture', 'architektur', 'pipeline', 'python', 'git', 'api', 'rest', 'agile', 'lean', 'consultant', 'training', 'data', 'performance', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 55/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements from the project offer:
  - Fine-tuning Transformer models and hands-on PyTorch (custom architectures & loss functions)
  - Handling multi-task / multi-label classification and severe class imbalance
  - Error analysis, data processing, label mapping, stratified splits
  - Deploying ML models as production REST APIs (Docker experience)
  - Strong software engineering (clean, modular Python) and collaboration with architects
  - Advanced degree in AI/NLP or related field

- Matches in the CV (strengths):
  - Advanced degree: Diplom-Informatiker (equivalent to a Master's) ‚Äî satisfies the education requirement.
  - Solid Python experience mentioned across projects and scripting (Python, bash) ‚Äî relevant for ML engineering.
  - Strong deployment / infra / production experience: Docker, Terraform, AWS (many services including Bedrock, SageMaker referenced indirectly), Lambda, API-related services ‚Äî good fit for production API integration and infra work.
  - DevOps/ML-ops strengths: IaC, Terraform, CI/CD (GitLab CI, Jenkins), Kubernetes (KCNA), monitoring basics ‚Äî supports productionizing models and automation.
  - Experience with generative-AI projects (LibreChat, AWS Bedrock, Ollama) and RAG/prompt engineering ‚Äî shows practical ML product experience and platform integration.
  - Proven software architecture and documentation skills ‚Äî aligns with requirement to document model, training and benchmarks.

- Partial / plausible matches:
  - Deploying ML as REST APIs: CV shows extensive AWS and API experience and work integrating AI platforms (Bedrock, LibreChat). He likely can integrate models into REST APIs, though direct examples of custom model-serving are not explicit.
  - Error analysis / data-processing: has general engineering and migration experience and describes data integration for RAG, but no explicit examples of detailed ML error analysis or stratified label-splitting procedures.

- Gaps / weaknesses relative to the role (critical):
  - No explicit hands-on PyTorch experience or examples of fine-tuning Transformer models (no mention of PyTorch, Hugging Face Transformers, training pipelines, GPU training, etc.).
  - No explicit demonstrable experience with multi-task or multi-label classification modeling, nor with techniques for severe class imbalance (focal loss, reweighting, up/down-sampling, synthetic data, curriculum learning) described in projects.
  - No explicit examples of implementing custom model architectures or custom loss functions in deep learning frameworks.

- Overall assessment / recommendation:
  - The candidate is a strong fit for the infra/productionization, cloud, DevOps and architecture parts of the brief and brings senior architecture experience plus practical generative AI product work (LibreChat, Bedrock). However, the role requires deep, hands-on transformer training and PyTorch model engineering expertise (including multi-label/multi-task and imbalance techniques) which are not evidenced in the CV.
  - If the assignment prioritizes production integration, cloud architecture and model deployment with some support for model training (e.g., using managed services like Bedrock or SageMaker), he can be a good fit. If the project needs a hands-on transformer researcher/engineer implementing custom PyTorch training loops and loss functions, additional candidate(s) with explicit PyTorch/transformer fine-tuning experience should be paired or preferred.

---


---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-02T23:08:44.350169

### Pre-Evaluation Phase
- **Score:** 42/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 42%. Found tags: ['aws', 'bedrock', 'ai', 'ki', 'llm', 'openai', 'generative', 'machine learning', 'ml', 'cloud', 'terraform', 'kubernetes', 'docker', 'monitoring', 'architect', 'architekt', 'cloud architect', 'architecture', 'architektur', 'devops', 'ci/cd', 'pipeline', 'jenkins', 'gitlab', 'automation', 'iac', 'python', 'bash', 'scripting', 'git', 'api', 'rest', 'integration', 'agile', 'lean', 'deployment', 'consultant', 'training', 'data', 'performance', 'freelance', 'projekt', 'project', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 58/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements from the project offer:
  - Fine-tuning Transformer models and hands-on PyTorch (custom architectures & loss functions)
  - Multi-task / multi-label classification and methods to handle severe class imbalance
  - Data processing, label mapping, stratified splitting and in-depth error analysis
  - Deploying models as production REST APIs (Docker, CI/CD, infra integration)
  - Strong software engineering: clean, modular Python code and collaboration with architects
  - Advanced degree in AI/NLP or related field

- Strong matches (supports fit):
  - Advanced degree (Diplom-Informatiker) ‚Äî meets educational requirement.
  - Broad, deep production/cloud/DevOps expertise: AWS (many services), Terraform, Docker, CI/CD, Kubernetes ‚Äî excellent for productionizing models and API integration.
  - Python used across many projects; experience with AI product integration (LibreChat, AWS Bedrock, RAG) ‚Äî shows practical generative-AI solutions experience.
  - Proven architecture, documentation and consulting skills ‚Äî useful for model design, documentation and collaboration with software architects.

- Partial matches (helpful but incomplete):
  - ML/AI product experience (Bedrock, SageMaker mention) suggests familiarity with managed model workflows but not necessarily hands‚Äëon model training.
  - General data/integration experience implies capability for data-processing and operational error analysis, but no explicit ML-focused examples.

- Gaps / risks (critical for this role):
  - No explicit hands-on PyTorch experience, no examples of fine-tuning Transformers, custom architectures or custom loss implementations.
  - No demonstrated work on multi-task or multi-label classification or specific techniques for severe class imbalance (focal loss, reweighting, synthetic sampling, curriculum learning, etc.).
  - No explicit examples of building training pipelines, GPU training, experiment tracking, or low-level model debugging/error-analysis in ML projects.

- Recommendation / summary:
  - Candidate is a strong fit for productionization, infra, API deployment and architecture parts of the brief and brings useful practical generative-AI product experience.
  - If the assignment prioritizes hands‚Äëon Transformer research/engineering (custom PyTorch training, imbalance handling, model internals), this candidate lacks demonstrated core skills and should be paired with or replaced by a specialist in PyTorch/Transformers.
  - If the assignment emphasizes integrating/operationalizing models (using managed services or a separate ML researcher on the team), the candidate is a good fit.

(Overall fit score: 58/100 ‚Äî strong infra/architect profile, limited evidence for low‚Äëlevel Transformer/PyTorch engineering.)

---
