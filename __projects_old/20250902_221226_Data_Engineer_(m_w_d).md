---
company: Hays AG
reference_id: '2915127'
scraped_date: '2025-09-02T22:12:26.978258'
source_url: https://www.freelancermap.at/projekt/data-engineer-m-w-d-2915127?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 40% fit score'
  state: rejected
  timestamp: '2025-09-02T22:13:28.212275'
title: Data Engineer (m/w/d)
---


# Data Engineer (m/w/d)
**URL:** [https://www.freelancermap.at/projekt/data-engineer-m-w-d-2915127?ref=rss](https://www.freelancermap.at/projekt/data-engineer-m-w-d-2915127?ref=rss)
## Details
- **Start:** ab sofort
- **Von:** Hays AG
- **Eingestellt:** 02.09.2025
- **Ansprechpartner:** Hays AG
- **Projekt-ID:** 2915127
- **Branche:** IT
- **Vertragsart:** Freiberuflich

## Schlagworte
Databricks, Pyspark, Python, Streaming, Apache Spark, Architektur, Partnerschaften, Continuous Integration, Vertragsverhandlung, Digitalisierung, Rekrutierung, Skalierbarkeit, Objektorientierte Software-Entwicklung, Performance-Tuning, Software Architecture, Verwaltungst√§tigkeiten, Refactoring

## Beschreibung
**Data Engineer (m/w/d)**

Referenz: -de
Beginn: asap
Dauer: 6 MM++

**Meine Aufgaben:**

- Entwicklung und Weiterentwicklung einer Streaming-Pipeline mit Databricks DLT unter Verwendung von PySpark
- Optimierung von PySpark-Code zur effizienten Verarbeitung von Streaming-Daten
- Pflege und Deployment eines Databricks Asset Bundles (DAB) zur strukturierten Verwaltung von Projektartefakten
- Weiterentwicklung und Implementierung objektorientierter Python-Komponenten, insbesondere im Kontext von Apache Spark
- Performance-Tuning und Code-Refactoring zur Sicherstellung von Skalierbarkeit und Wartbarkeit

**Meine Qualifikation:**

- Fundierte Erfahrung in der Entwicklung mit PySpark und Apache Spark
- Tiefes Verst√§ndnis von Streaming-Architekturen und deren Herausforderungen
- Erfahrung mit Databricks DLT und DAB
- Sicherer Umgang mit objektorientierter Programmierung in Python
- Kenntnisse in Deployment-Prozessen innerhalb von Databricks-Umgebungen
- TechStack: Databricks, Apache Spark, PySpark, Python, CI/CD f√ºr Databricks Deployment

**Meine Vorteile:**

- Internationaler Kunde
- Remote-M√∂glichkeit

**√úber uns:**
Der Bereich IT ist unsere Kernkompetenz, auf deren Grundlage sich Hays entwickelt hat. Wir sind das gr√∂√üte privatwirtschaftlich organisierte IT-Personaldienstleistungsunternehmen in Deutschland und haben f√ºr jede Karrierestufe das passende Angebot ‚Äì egal ob Sie an Vakanzen in agilen KMUs oder starken DAX-Konzernen interessiert sind. Wir beherrschen die komplette IT-Klaviatur von Support bis zur Softwarearchitektur oder Digitalisierung ‚Äì dank unseres umfangreichen Portfolios ist f√ºr jeden etwas dabei. So konnten wir in den vergangenen Jahrzehnten im Rahmen einer Life-Long-Partnerschaft unz√§hlige Fach- und F√ºhrungskr√§fte aus der IT dabei unterst√ºtzen, die Weichen f√ºr eine erfolgreiche Karriere zu stellen. Unser Beratungsteam ist spezialisiert und somit in der Lage, auf Ihre W√ºnsche und Vorstellungen einzugehen und Sie auf Bewerbungsgespr√§che und Vertragsverhandlungen bestens vorzubereiten. Probieren Sie es aus und erfahren Sie, was der Markt Ihnen zu bieten hat ‚Äì v√∂llig kostenfrei, diskret und unverbindlich! Wir freuen uns auf Sie.

**Mein Kontakt bei Hays:**

Mein Ansprechpartner:
Vincent Ru√ü

Referenznummer:

Kontakt aufnehmen:
Email:

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-02T22:13:28.207997

### Pre-Evaluation Phase
- **Score:** 19/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 19%. Found tags: ['ai', 'architect', 'architekt', 'architecture', 'architektur', 'ci/cd', 'pipeline', 'python', 'git', 'rest', 'integration', 'agile', 'continuous', 'deployment', 'beratung', 'data', 'streaming', 'skalierbarkeit', 'performance', 'remote', 'freelance', 'projekt']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 40/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key project requirements: Databricks (DLT, DAB), PySpark/Apache Spark, streaming pipeline development and optimization, object-oriented Python, Databricks deployment/CI-CD, performance tuning and scalability.
- Strong matches in CV: long-term architecture experience, solid Python and Bash usage across projects, CI/CD experience (GitLab CI/CD, Jenkins, AWS Code*), cloud-native and streaming-service knowledge in AWS training (Kinesis, EMR mentioned), extensive performance-tuning and large-scale system experience, Terraform/Docker/Kubernetes/DevOps skills to support deployments.
- Partial/inferred matches: candidate has AWS big-data service exposure listed in training (EMR, Kinesis) which indicates familiarity with streaming concepts and big-data operations, and strong general data/architecture skills that are transferable to Databricks/Spark ecosystems.
- Gaps / risks: no explicit hands-on experience with Apache Spark or PySpark shown; no explicit Databricks experience (DLT, DAB) in projects; no clear examples of object-oriented Python components developed for Spark or of Databricks-specific CI/CD/deployment pipelines.
- Recommendation / conclusion: candidate has many transferable skills (cloud, Python, CI/CD, streaming concepts, performance tuning) and could ramp up to Databricks/PySpark quickly, but for an immediate plug-and-play Data Engineer role that requires proven Databricks + PySpark experience the fit is limited without short ramp-up/training‚Äîhence a moderate-low fit score of 40/100.

---
