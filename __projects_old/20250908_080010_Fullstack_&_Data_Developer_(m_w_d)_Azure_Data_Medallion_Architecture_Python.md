---
company: Circle8 Deutschland
reference_id: '2916501'
scraped_date: '2025-09-08T08:00:10.921003'
source_url: https://www.freelancermap.de/projekt/fullstack-developer-m-w-d-azure-data-python-und-sql-medallion-architecture?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 50% fit score'
  state: rejected
  timestamp: '2025-09-08T08:01:57.841164'
title: Fullstack & Data Developer (m/w/d) Azure Data / Medallion Architecture / Python
---


# Fullstack & Data Developer (m/w/d) Azure Data / Medallion Architecture / Python
**URL:** [https://www.freelancermap.de/projekt/fullstack-developer-m-w-d-azure-data-python-und-sql-medallion-architecture?ref=rss](https://www.freelancermap.de/projekt/fullstack-developer-m-w-d-azure-data-python-und-sql-medallion-architecture?ref=rss)
## Details
- **Start:** keine Angabe
- **Von:** Circle8 Deutschland
- **Eingestellt:** 06.09.2025
- **Ansprechpartner:** Mariusz Domanski
- **Projekt-ID:** 2916501
- **Branche:** IT
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 100
                                                % Remote

## Schlagworte
Architektur, Microsoft Azure, It-Governance, SQL, Databricks, Datenbanken, Python, Datenaufnahme, Third Normal Form, Data Analysis, Anlagenverwaltung, ETL, Data Mining, Relationale Datenbanken, Machine Learning, Sql Azure, Qualit√§tsmanagement, Power Bi, Schreiben von Dokumentation, Lagerverwaltung, Workflows, Modellierungsf√§higkeiten, Verwaltungst√§tigkeiten, Azure Data Factory, Snowflake, Data Lineage

## Beschreibung
Leistungsbeschreibung

**Projektbeschreibung:**

Wir befinden uns derzeit in der Modernisierung unseres intern entwickelten**Asset Management Tools**, das aktuell auf einem klassischen relationalen Datenbanksystem (RDBMS) basiert. Das Tool konsolidiert Daten aus verschiedenen heterogenen Quellen zur Verwaltung finanzieller und operativer Assets und umfasst Module f√ºr Tracking, Reporting und Analyse.

Derzeit werden alle Datenaufnahme-, Verarbeitungs- und Speicherprozesse direkt innerhalb einer**Azure SQL Server**-Datenbank durchgef√ºhrt.

**Ziel:**

Mit zunehmender Skalierung m√∂chten wir die Architektur auf eine moderne, skalierbare und wartbare**Datenplattform auf Basis von Databricks**√ºberf√ºhren. Unser gew√§hlter Ansatz ist die**Medallion-Architektur**(Bronze-, Silber- und Gold-Layer), die eine standardisierte Datenaufnahme, -transformation und -bereitstellung erm√∂glicht und gleichzeitig zuk√ºnftige analytische sowie Machine-Learning-Anwendungen unterst√ºtzt. Zus√§tzlich sollen**Power BI Reports**direkt an diese Plattform angebunden werden.
Anforderungen

**Aufgaben:**

- Unterst√ºtzung beim Anschluss verschiedener Quellsysteme an die zentralen IT-Governance-Tools.
- Zusammenarbeit mit IT-Produktverantwortlichen der Quellsysteme sowie dem Team der IT-Governance-Tools, um Daten in das**Common Data Model**zu transformieren und zentral verf√ºgbar zu machen.
- Konzeption und Implementierung von Schnittstellen f√ºr**Datenextraktion, -transformation und -ladung (ETL/ELT)**zu und von den IT-Governance-Tools.
- Design und Umsetzung von**Medallion-Pipelines**(Ingest ? Refine ? Serve) mit Databricks.
- Optimierung von Datenbankschemata f√ºr**Analytics**(Star-/Snowflake-Schema) sowie f√ºr operative Lesezugriffe.
- Implementierung von**Unity Catalog**, Datenherkunft (Data Lineage), Zugriffskontrollen, PII-Handling und Qualit√§tspr√ºfungen.
- Entwurf robuster**Datenbankschemata**und**Medallion-Datenpipelines**f√ºr Bestands und Inventarprozesse.
- Erstellung, Pflege und regelm√§√üige Ver√∂ffentlichung von**technischer und funktionaler Dokumentation**.

**Zus√§tzliche Kompetenzen / Profilanforderungen:**

- Nachweisbare Erfahrung mit dem**Microsoft Azure Technologie-Stack**, insbesondere**Azure Data Factory (ADF)**und**Azure Databricks**.
- Fundierte Kenntnisse in**dimensionalem Modellieren**und**3NF-Modellierung**; sicherer Umgang mit**CDC**, versp√§teten/inkonsistenten Daten und**Schema-Evolution**.
- Nachweisbare Erfahrung in der**Entwicklung mit Python und SQL**.
- Erfahrung in der Implementierung der**Medallion-Architektur**,**Delta Lake**,**Unity Catalog**, Workflows, SQL Warehouse und Delta Live Tables.
- Starke analytische F√§higkeiten und strukturiertes Denken.
- Ausgepr√§gte Probleml√∂sungskompetenz und F√§higkeit zur**teamorientierten Zusammenarbeit**.
- Flie√üende Englischkenntnisse.
√úber den Auftraggeber

Start: ASAP

Ende: 31.01.2026 plus Option auf Verl√§ngerung

Auslastung: ca. 30 Stunden pro Woche

Onsite Stunden: 0

Offsite Stunden: 600

Standort: Hannover | 100 % remote

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-08T08:01:57.837496

### Pre-Evaluation Phase
- **Score:** 17/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 17%. Found tags: ['ai', 'ki', 'machine learning', 'ml', 'architect', 'architekt', 'architecture', 'architektur', 'pipeline', 'python', 'sql', 'data', 'analytics', 'etl', 'remote', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 50/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key project requirements:
  - Microsoft Azure Data stack: Azure Databricks, Azure Data Factory (ADF), SQL Azure
  - Implementation of Medallion Architecture (Bronze/Silver/Gold), Delta Lake, Delta Live Tables, Unity Catalog
  - Strong Python and SQL development skills; ETL/ELT, CDC, schema evolution, data lineage, PII handling
  - Dimensional modelling (Star/Snowflake) and analytics/ML enablement; Power BI integration
  - Governance, access controls, documentation, team collaboration and fluent English

- Strong matches from CV:
  - Deep, long-term architecture experience and proven track record building scalable data/platform solutions
  - Cloud platform expertise (extensive AWS knowledge, Landing Zone, multi-account, security, governance)
  - Solid experience with data migrations, ETL-like projects, CI/CD for data platforms, automation (Terraform, GitLab CI/CD, CloudFormation concepts)
  - Python and SQL experience across many projects; experience with data warehousing concepts (SAP BW/4HANA engagement implies dimensional modelling awareness)
  - Good experience in security, IAM, audit/compliance and documenting solutions (Live-Scripting repository + extensive documentation practice)
  - Fluent English and experience collaborating with product teams and operations remotely

- Gaps / risks vs. job specifics:
  - No explicit Microsoft Azure Data experience listed (Azure Databricks, ADF, SQL Azure) ‚Äî project requires hands-on Azure stack
  - No explicit Databricks/Delta Lake/Unity Catalog/Delta Live Tables experience documented
  - Power BI experience not shown
  - CDC, schema-evolution and Databricks-specific data-governance implementations not evidenced
  - Most cloud- and data-platform work is AWS/Oracle/WebLogic-centric; migration to an Azure/Databricks-first platform would require ramp-up time

- Overall assessment:
  - Candidate is a strong senior architect/engineer with transferable cloud, data-migration, ETL and modelling skills and excellent governance/security background. However, the role requires specific hands-on Azure Databricks and Delta/Unity-ecosystem experience which is not present on the CV. If the client can accept a senior architect who will ramp up Azure/Databricks quickly (leveraging strong AWS/cloud & data-platform skills), this candidate is a reasonable fit. For a plug-and-play Databricks specialist the fit is weak.

- Recommendation:
  - Fit score 50/100 reflects high architectural and data experience but missing critical, role-specific Azure/Databricks skills.

---
