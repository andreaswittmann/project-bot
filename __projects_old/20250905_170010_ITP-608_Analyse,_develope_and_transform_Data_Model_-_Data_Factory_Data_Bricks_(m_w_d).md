---
company: IT-P GmbH Information Technology-Partner
reference_id: '2916361'
scraped_date: '2025-09-05T17:00:10.831264'
source_url: https://www.freelancermap.de/projekt/itp-608-analyse-develope-and-transform-data-model-data-factory-data-bricks-m-w-d?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 50% fit score'
  state: rejected
  timestamp: '2025-09-05T17:03:01.368471'
title: 'ITP-608: Analyse, develope and transform Data Model - Data Factory / Data
  Bricks (m/w/d)'
---


# ITP-608: Analyse, develope and transform Data Model - Data Factory / Data Bricks (m/w/d)
**URL:** [https://www.freelancermap.de/projekt/itp-608-analyse-develope-and-transform-data-model-data-factory-data-bricks-m-w-d?ref=rss](https://www.freelancermap.de/projekt/itp-608-analyse-develope-and-transform-data-model-data-factory-data-bricks-m-w-d?ref=rss)
## Details
- **Start:** ab sofort
- **Von:** IT-P GmbH Information Technology-Partner
- **Auslastung:** 100% (5 Tage pro Woche)
- **Eingestellt:** 05.09.2025
- **Ansprechpartner:** Marcel Paul
- **Projekt-ID:** 2916361
- **Branche:** Energiewirtschaft
- **Vertragsart:** Freiberuflich

## Schlagworte
It-Governance, Databricks, Microsoft Azure, Datenbanken, Relationale Datenbanken, SQL, Third Normal Form, Architektur, Anlagenverwaltung, Data Transformation, Data Mining, Python, Machine Learning, Sql Azure, Scrum, Lagerverwaltung, Workflows, Azure Data Factory, Snowflake, Data Lineage

## Beschreibung
Start: 01.09.2025
Ende: 31.01.2026
Einsatzort: Hannover
Weitere Hinweise: Bei den Konditionen ist eine Differenz 18‚Ç¨/h von Onsite zum Offsite Stundensatz n√∂tig. Die geplante Aufteilung der Stunden ist wie genannt, kann aber abweichen.
Max. Stundensatz remote: 75‚Ç¨/h ‚Äì 87/h
Offsite: 600h
Onsite: 0h
Reisezeiten: 0h
Anzahl Personen: 1

Job-Details:
We are in the process of modernizing our internally developed Asset Management Tool, which is currently backed by a traditional RDBMS database system. The tool consolidates data from multiple heterogeneous sources to manage financial or operational assets, and includes modules for tracking, reporting, and analysis. At present, all the data ingestion, processing, and storage are managed directly within a relational database Azure sql server.
Goal:
As we scale, we have identified the need to transition this architecture to a modern, scalable, and maintainable data platform using Databricks. Our chosen architectural approach is the Medallion Architecture (Bronze, Silver, Gold layers), which will help us standardize data ingestion, transformation, and consumption while supporting future analytical and machine learning use cases. Also connecting the Power BI reports directly with this.

Tasks:
‚Ä¢ You facilitate the process of connecting various source systems to the central IT governance tools
‚Ä¢ You work together with the IT product owners of the source systems and the team responsible for the IT governance tools to transform the data into the common data model of the customer and make it centrally available
‚Ä¢ You design and implement interfaces for data extraction, data transformation and data loading to and from the IT governance tools
‚Ä¢ You design and implement Medallion-layered pipelines (ingest ? refine ? serve) using Databricks
‚Ä¢ You optimize the schema for analytics (star/snowflake) and for operational read patterns
‚Ä¢ You implement Unity Catalog, data lineage, access controls, PII handling, and quality checks
‚Ä¢ You‚Äôll design Medallion (Bronze/Silver/Gold) data pipelines, craft robust database schemas for inventory operations
‚Ä¢ You create technical and functional documentation and publish them regularl

Skills:
‚Ä¢ You have proven experience with the Microsoft Azure technology stack, especially Azure Data Factory (ADF) and Azure Databricks
‚Ä¢ You have strong dimensional and 3NF modeling knowledge; are comfortable with CDC, late/dirty data, and schema evolution.
‚Ä¢ You have proven experience developing with Python and SQL
‚Ä¢ You have experience implementing Medallion architecture, Delta Lake, Unity Catalog, Workflows, SQL Warehouse, and Delta Live Tables.
‚Ä¢ You show strong analytical and structured thinking abilities
‚Ä¢ You show strong problem-solving skills and the ability to work collaboratively in a team environment
‚Ä¢ You are fluent in English language

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-05T17:03:01.364647

### Pre-Evaluation Phase
- **Score:** 17/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 17%. Found tags: ['ai', 'ki', 'machine learning', 'architect', 'architekt', 'architecture', 'architektur', 'pipeline', 'python', 'database', 'sql', 'scrum', 'data', 'analytics', 'remote', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 50/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key project requirements: Azure (Azure Data Factory), Azure Databricks (Delta Lake, Delta Live Tables, Unity Catalog, Workflows, SQL Warehouse), Medallion architecture (Bronze/Silver/Gold), dimensional and 3NF modelling, CDC/schema evolution, Python & SQL, data lineage/PII handling/data quality, Power BI/Snowflake familiarity, ability to work with source-system owners and IT governance.

- Strong matches from CV: ~30 years of architecture/consulting experience; extensive cloud architecture skills (deep AWS certs and hands‚Äëon Terraform, Docker, Kubernetes); solid Python and SQL experience; proven RDBMS and data migration work (Oracle Forms/Reports, Oracle DB, SAP BW/4HANA architecture) indicating data‚Äëwarehousing and modelling understanding; experience with integration/ETL, automation (IaC), documentation, and stakeholder collaboration.

- Gaps / risks: No explicit hands‚Äëon experience listed with Microsoft Azure, Azure Data Factory, Azure Databricks, Delta Lake, Unity Catalog or Delta Live Tables; Power BI and Snowflake not mentioned; Medallion architecture and CDC/streaming specifics not shown as implemented (though conceptual DW experience exists). Immediate start and short project window increase ramp‚Äëup risk for platform‚Äëspecific features.

- Assessment / recommendation: Candidate brings excellent architectural, data modelling and cloud migration skillsets and can likely adapt quickly. However, the role requires immediate, hands‚Äëon Azure Databricks/ADF expertise; lacking explicit experience there lowers fit for a plug‚Äëand‚Äëplay assignment. Best use: lead architecture/ modelling and migration planning, with a Databricks/ADF specialist for implementation or allow a short ramp‚Äëup period.

- Overall fit score: 50/100 (strong transferable skills and DW knowledge, but missing core platform experience required for immediate hands‚Äëon delivery).

---
