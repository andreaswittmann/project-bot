---
company: Spirit Experts GmbH
reference_id: '2916510'
scraped_date: '2025-09-08T08:00:09.237216'
source_url: https://www.freelancermap.de/projekt/azure-data-platform-engineer-modernisierung-asset-management-system-m-w-d?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 45% fit score'
  state: rejected
  timestamp: '2025-09-08T08:01:39.035629'
title: Azure Data Platform Engineer - Modernisierung Asset Management System (m/w/d)
---


# Azure Data Platform Engineer - Modernisierung Asset Management System (m/w/d)
**URL:** [https://www.freelancermap.de/projekt/azure-data-platform-engineer-modernisierung-asset-management-system-m-w-d?ref=rss](https://www.freelancermap.de/projekt/azure-data-platform-engineer-modernisierung-asset-management-system-m-w-d?ref=rss)
## Details
- **Start:** ab sofort
- **Von:** Spirit Experts GmbH
- **Auslastung:** 80% (4 Tage pro Woche)
- **Eingestellt:** 08.09.2025
- **Ansprechpartner:** Benjamin Heunemann
- **Projekt-ID:** 2916510
- **Branche:** Energiewirtschaft
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 100
                                                % Remote

## Schlagworte
Architektur, Microsoft Azure, Databricks, Data Analysis, Anlagenverwaltung, Datenbank Design, SQL, Third Normal Form, ETL, Datenqualit√§t, Skalierbarkeit, Python, Schreiben von Dokumentation, Datenverarbeitung, Azure Data Factory, Snowflake, Data Lineage

## Beschreibung
Unterst√ºtze uns bei der Transformation eines etablierten Asset Management Tools von einer klassischen SQL-Datenbankarchitektur hin zu einer modernen, cloud-nativen Datenplattform. Im Fokus steht die Implementierung einer skalierbaren Medallion-Architektur bei einem Kunden mit Databricks, die sowohl operative Prozesse als auch zuk√ºnftige Analytics- und ML-Anwendungen optimal unterst√ºtzt.

Aufgaben:
‚Ä¢ Konzeption und Implementierung von ETL/ELT-Pipelines zur Anbindung verschiedener Quellsysteme
‚Ä¢ Design und Umsetzung der Medallion-Architektur (Bronze/Silver/Gold-Layer) mit Azure Databricks
‚Ä¢ Entwicklung robuster Datenintegrationsl√∂sungen f√ºr heterogene Datenquellen
‚Ä¢ Optimierung von Datenschemata f√ºr Analytics (Star/Snowflake) und operative Zugriffe
‚Ä¢ Implementierung von Unity Catalog, Data Lineage und Zugriffskontrollen
‚Ä¢ Integration von Power BI Reports in die neue Datenplattform
‚Ä¢ Erstellung und Pflege technischer sowie funktionaler Dokumentation

Profil:
‚Ä¢ Mehrj√§hrige Erfahrung mit Microsoft Azure Data Stack (Azure Data Factory, Databricks)
‚Ä¢ Expertise in Python und SQL f√ºr Datenverarbeitung und -transformation
‚Ä¢ Fundierte Kenntnisse in dimensionaler Modellierung und 3NF-Datenbankdesign
‚Ä¢ Praktische Erfahrung mit Medallion-Architektur, Delta Lake und Unity Catalog
‚Ä¢ Umgang mit CDC, Schema-Evolution und Data Quality Management
‚Ä¢ Starke analytische F√§higkeiten und strukturierte Arbeitsweise
‚Ä¢ Teamorientierte Arbeitsweise und ausgepr√§gte Probleml√∂sungskompetenz
‚Ä¢ Flie√üende Englischkenntnisse in Wort und Schrift

Projekt-Eckdaten:
Remote/Onsite: 100% Remote
Projektstart: ab sofort
Projektdauer: 4-5 Monate
Option auf Verl√§ngerung: Ja

Wichtig f√ºr Bewerbende:
Bitte senden Sie Ihren CV (deutsch), Netto-Stundensatz und Verf√ºgbarkeit via E-Mail an

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-08T08:01:39.031018

### Pre-Evaluation Phase
- **Score:** 14/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 14%. Found tags: ['ai', 'ml', 'cloud', 'architekt', 'architektur', 'pipeline', 'python', 'sql', 'integration', 'data', 'analytics', 'etl', 'skalierbarkeit', 'remote', 'freelance', 'projekt']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 45/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key project requirements:
  - Azure Data Stack (Azure Data Factory, Databricks), Delta Lake/Medallion-Architecture, Unity Catalog
  - Python and SQL for ETL/ELT; CDC, schema evolution, data quality, data lineage
  - Dimensional modelling / 3NF and Power BI integration
  - Documentation, team work, fluent English

- Strong matches from CV:
  - Solid Python and SQL experience (many migrations, Oracle DB, Forms/Reports projects)
  - Extensive ETL/migration and integration background (Oracle Service Bus, OSB, many migrations to cloud)
  - Strong cloud architecture and DevOps skills (AWS Landing Zones, Terraform, CI/CD, IaC) and security experience
  - Experience with data-warehouse style projects (SAP BW/4HANA architecture design) ‚Äî indicates familiarity with dimensional modelling concepts
  - Good documentation and consulting experience; fluent English

- Important gaps / risks:
  - No explicit hands-on experience with Microsoft Azure Data Factory or Azure Databricks / Delta Lake / Unity Catalog
  - No explicit Power BI, Snowflake or Databricks-specific tooling experience (Unity Catalog, Delta Lake features, Medallion implementation)
  - CDC, schema-evolution and Data Lineage tooling not demonstrably shown (only general migration experience)
  - Candidate is strongly AWS-centric; ramp-up to Azure Databricks would be needed for an immediate, hands-on role

- Conclusion and recommendation:
  - Andreas is a strong senior cloud/integration architect with relevant migration, ETL and data-warehouse background and the ability to learn fast. However, for a short-term, hands-on Azure Databricks/ADF implementation role requiring immediate specialist skills (Databricks, Delta Lake, Unity Catalog, Power BI), he is not an ideal fit without a short ramp-up period.
  - Fit score 45/100 reflects good overlap on ETL, Python/SQL, architecture and migration experience but significant missing specialist Azure/Databricks skills for this assignment.

---
