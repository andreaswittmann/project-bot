---
company: Tergos GmbH
reference_id: '2916631'
scraped_date: '2025-09-08T12:00:09.770863'
source_url: https://www.freelancermap.de/projekt/devops-engineer-m-w-d-mit-kafka-erfahrung?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 45% fit score'
  state: rejected
  timestamp: '2025-09-08T12:04:06.775008'
title: DevOps Engineer (m/w/d) mit Kafka-Erfahrung
---


# DevOps Engineer (m/w/d) mit Kafka-Erfahrung
**URL:** [https://www.freelancermap.de/projekt/devops-engineer-m-w-d-mit-kafka-erfahrung?ref=rss](https://www.freelancermap.de/projekt/devops-engineer-m-w-d-mit-kafka-erfahrung?ref=rss)
## Details
- **Start:** 10.2025
- **Von:** Tergos GmbH
- **Eingestellt:** 08.09.2025
- **Ansprechpartner:** Miriam Jedelhauser
- **Projekt-ID:** 2916631
- **Branche:** IT
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 80
                                                % Remote

## Schlagworte
Apache Kafka, Microsoft Azure, Architektur, Cloud Computing, Datenbanken, Continuous Integration, Data Integration, Devops, Postgresql, Microsoft Sql-Server, Nexus 1000V, Oracle Financials, Power Bi, SAP Applications, Streaming, Daten- / Datensatzprotokollierung, Cloud Platform, Gitlab, Data Lake, Kubernetes, Devsecops, Confluent, Microservices

## Beschreibung
F√ºr einen Hamburger Kunden suchen ich einen erfahrenen Dev(Sec)Ops Engineer mit tiefgreifenden Kafka Kenntnissen.

Kontext:
Im Rahmen des Aufbaus einer unternehmensweiten Datenplattform soll Apache Kafka als zentrale Event-Streaming-Plattform f√ºr die Integration von Quell- und Zielsystemen etabliert werden. Das Kafka-Cluster dient als Backbone f√ºr Datenintegration, Echtzeitverarbeitung und die Bereitstellung analytischer und operativer Datenstr√∂me (z.B. f√ºr Azure Synapse, Data Lake, Power BI, Microservices).

Must Haves:
‚Ä¢ DevSecOps
‚Ä¢ Kafka
‚Ä¢ Azure / AKS
‚Ä¢ Kubernetes
‚Ä¢ GitLab / Piplines / CI/CD
‚Ä¢ Nexus
‚Ä¢ Cloud native Dev
‚Ä¢ AuthN & AuthZ mit Entra ID

Anforderungen:
‚Ä¢ Mehrj√§hrige praktische Erfahrung mit Apache Kafka in produktiven Umgebungen
‚Ä¢ Fundiertes Wissen zu Kafka-Architektur, Partitionierung, Consumer-Gruppen, Delivery-Semantics etc.
‚Ä¢ Erfahrung mit Kafka Connect, inkl. Konnektoren f√ºr Datenbanken (z.B. PostgreSQL, Oracle, SQL-Server) und idealerweise SAP
‚Ä¢ Erfahrung mit Schema Management (Confluent Schema Registry)
‚Ä¢ Kenntnisse in der Anbindung von Kafka an Cloud-Plattformen (Azure, idealerweise auch Event Hubs)
‚Ä¢ Erfahrung im Aufbau von robustem Monitoring und Logging f√ºr Kafka-Umgebungen

Start: ab Oktober 2025
Auslastung: 80-100%
Ort: Hamburg, hybrid (Onboarding sowie einzelne Meetings vor Ort)

Ich freue mich auf Deine Bewerbung mit einer aktuellen Projekt√ºbersicht und Deinem Stundensatz an

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-08T12:04:06.771287

### Pre-Evaluation Phase
- **Score:** 18/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 18%. Found tags: ['ai', 'cloud', 'kubernetes', 'monitoring', 'architekt', 'architektur', 'devops', 'ci/cd', 'gitlab', 'git', 'oracle', 'sql', 'integration', 'iam', 'continuous', 'data', 'streaming', 'remote', 'hybrid', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 45/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key project requirements:
  - DevSecOps, Kafka (production experience, Connect, Schema Registry/Confluent)
  - Azure / AKS, Kubernetes
  - GitLab CI/CD / pipelines
  - Nexus, cloud-native development
  - AuthN/AuthZ with Entra ID (Azure AD)
  - DB connectors (Postgres, Oracle, SQL-Server), monitoring/logging for Kafka

- Matches in CV:
  - DevOps / DevSecOps mindset and security experience (WS‚ÄëSecurity, SAML, IAM workshops, security hardening)
  - Kubernetes skills and recent KCNA certification; hands‚Äëon with k3s, EKS, Helm, ArgoCD
  - CI/CD experience including explicit GitLab CI/CD work (2019 migration) and Jenkins experience
  - Terraform, IaC, Docker, AWS cloud‚Äënative design and multi-account Landing Zone experience
  - Monitoring basics: exposure to Prometheus and Grafana (tested/used)
  - Strong DB and middleware background (extensive Oracle experience) and broad AWS service knowledge (including Amazon MSK listed among AWS services)
  - Based in Hamburg (matches location/hybrid requirement) and self-employed (flexible start/engagement)

- Gaps vs. requirements:
  - No evidence of focused, multi‚Äëyear hands‚Äëon Apache Kafka production projects (core requirement). Kafka only appears implicitly in the AWS services list (Amazon MSK) ‚Äî not as delivered project work.
  - No explicit Kafka Connect / connector implementations for Postgres/Oracle/SQL‚ÄëServer, nor Confluent Schema Registry experience documented.
  - Azure / AKS and Entra ID (AuthN/AuthZ) experience are not documented; the CV is AWS‚Äëcentric.
  - Nexus repository experience not shown.
  - No explicit integration work with Azure Synapse / Event Hubs / Power BI or streaming pipelines into Azure analytics stack.

- Overall assessment:
  - The candidate is a seasoned DevOps/Cloud architect with strong Kubernetes, IaC, security and CI/CD credentials and is geographically suitable.
  - However, the project‚Äôs must‚Äëhave of deep, demonstrable Kafka experience (including Connect, Schema Registry/Confluent and DB connectors) and Azure/AKS-specific skills are lacking in the CV. These are high‚Äërisk gaps for a Kafka‚Äëcentric, Azure‚Äëbased data platform role.

- Recommendation: consider the candidate if the client is open to a hybrid role where Andreas leads architecture, DevSecOps and Kubernetes/Terraform work while pairing with a Kafka/Confluent specialist and an Azure engineer. If pure, hands‚Äëon Kafka+AKS delivery is required from day one, look for stronger Kafka+Azure evidence.

---
