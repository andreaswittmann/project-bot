---
company: Wavestone Germany
reference_id: '2916634'
scraped_date: '2025-09-08T12:00:08.336329'
source_url: https://www.freelancermap.de/projekt/senior-data-engineer-m-w-d-2916634?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 65% fit score'
  state: rejected
  timestamp: '2025-09-08T12:04:58.692807'
title: Senior Data Engineer (m/w/d)
---


# Senior Data Engineer (m/w/d)
**URL:** [https://www.freelancermap.de/projekt/senior-data-engineer-m-w-d-2916634?ref=rss](https://www.freelancermap.de/projekt/senior-data-engineer-m-w-d-2916634?ref=rss)
## Details
- **Start:** 20.09.2025
- **Von:** Wavestone Germany
- **Eingestellt:** 08.09.2025
- **Ansprechpartner:** Christian Bock
- **Projekt-ID:** 2916634
- **Vertragsart:** Freiberuflich
- **Einsatzart:** Auslastung: 95 %

## Schlagworte
Amazon Web Services, Beratung, Python, Redis, Logistikprozesse, KÃ¼nstliche Intelligenz, Big Data, GeschÃ¤ftsanforderungen, Cloud Computing, Datenvalidierung, Datenmodell, DatenqualitÃ¤t, Relationale Datenbanken, Apache Hadoop, MultidisziplinÃ¤ren Ansatz, Umstrukturierung, Streaming, VerwaltungstÃ¤tigkeiten, Datenstrategie, Kubernetes, Apache Flink

## Beschreibung
Wir suchen einen Senior Data Engineer fÃ¼r das Projekt 'KI in der Disposition' zur Weiterentwicklung von Datenpipelines und datengetriebenen Komponenten zur UnterstÃ¼tzung von Dispositionsentscheidungen

Weitere Rahmendaten
Einsatzart:
Auslastung: 95 %
Branche: Transport - Travel - Logistik

Ihre Aufgaben
â€¢ Technische Weiterentwicklung unserer Datenpipelines (Batch und Streaming) in der AWS Cloud im Python Code
â€¢ Sicherstellung der DatenqualitÃ¤t und Entwurf relationaler und nicht-relationaler Datenmodelle zur ErfÃ¼llung der GeschÃ¤ftsanforderungen
â€¢ Entwicklung und Betrieb von internen und externen Schnittstellen zur Bereitstellung von Datenprodukten
â€¢ Weiterentwicklung unserer Datenstrategie
â€¢ Beratung der Projektmitglieder zum Einsatz unterschiedlicher AWS-Technologien und zur DatenqualitÃ¤tssicherung
â€¢ Entwicklung datengesteuerter Softwarekomponenten zur Optimierung von Dispositionsentscheidungen und verantwortet ein mÃ¶glichst reibungsloses, effizientes und fehlerfreies Bereitstellen der Datenquellen und deren Aggregationen

Muss-Anforderungen
â€¢ Umfangreiche und aktuelle Kenntnisse im Aufbau und in der Restrukturierung von Datenarchitekturen, insbesondere relationaler und nicht-relationaler Datenbanken, nachgewiesen durch Berufserfahrung in mindestens 3 Kundenprojekten
â€¢ Praktische Erfahrungen mit Big-Data-Tools (z.B. Redis, Flink, Hadoop), nachgewiesen in mindestens 3 Projekten
â€¢ Praktische Erfahrung mit Frameworks zur Datenvalidierung (z.B. Great Expectations, Pandera oder TFDV), mindestens 3 Jahre Berufserfahrung
â€¢ Erfahrung in der Verwaltung von Kubernetes-Clustern, nachgewiesen in mindestens 3 Projekten
â€¢ Umfangreiche Erfahrung in der Python-Programmierung, mindestens 10 Jahre Berufserfahrung

Kann-Anforderungen
â€¢ Erfahrungen in anderen GroÃŸkonzernen, nachgewiesen durch Projekte in mindestens einem weiteren Konzern
â€¢ Erfahrung im Sektor Transport und Logistik, nachweislich durch mindestens ein Datenprojekt innerhalb des Sektors
â€¢ FlieÃŸend in Deutsch (C1) mit Nachweis der Qualifikation Ã¼ber Zertifikat/Projekterfahrung
â€¢ Erfahrung mit Great Expectations, nachgewiesen durch Projekterfahrung
â€¢ Erfahrung mit Lambda, nachgewiesen durch Projekterfahrung
â€¢ Erfahrung mit CDK, nachgewiesen durch Projekterfahrung
â€¢ Erfahrung mit Athena, nachgewiesen durch Projekterfahrung
â€¢ Erfahrung mit Redis, nachgewiesen durch Projekterfahrung

Weitere Informationen
Einsatzort Berlin (onshore); teilweise Verteilung zwischen Remote und Onsite mÃ¶glich; regelmÃ¤ÃŸige Onsite-AktivitÃ¤ten fÃ¼r Entwurf und gemeinsame Workshops vor Ort vorgesehen; Einsatzzeitraum 20.09.2025 - 31.12.2025; Gesamtaufwand 70 Personentage, davon 7 Onsite-PT

Letâ€™s power the future together
Vom Business Case bis hin zur Umsetzung: Als fÃ¼hrendes Beratungsunternehmen fÃ¼r strategische Transformationen sind wir vertrauenswÃ¼rdiger Partner fÃ¼r unsere Kunden - und fÃ¼r unsere Mitarbeitenden. Verantwortungsvoll, leistungsstark und immer mit dem Menschen im Fokus. #WeAreWavestone
Mit unserem 360Â°-Portfolio an Beratungsleistungen verbinden wir erstklassige Branchenexpertise mit einem breiten Spektrum an branchenÃ¼bergreifenden Kompetenzen, arbeiten interdisziplinÃ¤r und denken Ã¼ber den Tellerrand hinaus. Unseren Partnerunternehmen und Freelancern:Freelancerinnen kÃ¶nnen wir so umfassende Perspektiven innerhalb unserer eigenen Projekte bieten und unterstÃ¼tzen als langjÃ¤hriger Rahmenvertragspartner bei der Besetzung von Projektvakanzen â€“ zeitnah und direkt.

Wir freuen uns auf Ihre Kontaktaufnahme!

Ihr direkter Ansprechpartner bei Wavestone
Christian Bock
Telefon:
E-Mail:

---

## ðŸ¤– AI Evaluation Results

**Evaluation Timestamp:** 2025-09-08T12:04:58.688766

### Pre-Evaluation Phase
- **Score:** 19/100
- **Threshold:** 10/100
- **Result:** âœ… Passed
- **Rationale:** Score: 19%. Found tags: ['aws', 'amazon', 'ai', 'ki', 'cloud', 'kubernetes', 'architekt', 'architektur', 'pipeline', 'python', 'rest', 'beratung', 'workshop', 'data', 'big data', 'streaming', 'remote', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 65/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** âŒ REJECTED

#### Detailed Rationale
- Key requirements (from project):
  - Strong Python (>=10 years) for batch & streaming pipelines
  - Experience building/restructuring data architectures (relational & non-relational) across >=3 client projects
  - Practical experience with big-data tools (Redis, Flink, Hadoop) in >=3 projects
  - Experience with data-validation frameworks (Great Expectations, Pandera, TFDV) ~3 years
  - Kubernetes cluster management experience proven in >=3 projects
  - AWS experience and ability to advise on AWS technologies (streaming, analytics)

- Strong matches from CV:
  - Python: many projects list Python (automation, integration, CI/CD, tooling) across decades â†’ meets 10+ years requirement.
  - Data architecture / databases: extensive enterprise architecture work (Oracle DB, RDS, migrations, SAP on AWS, ETL/EMR listed in training) and multiple migrations/landing-zone projects â†’ strong experience designing/resctructuring architectures.
  - AWS: multiple professional AWS certs (Solutions Architect Pro/Assoc, SysOps, Developer, AI Practitioner) and numerous AWS services listed and used in projects (Landing Zone, EC2, CloudFormation, etc.) â†’ strong cloud competency.
  - DevOps / IaC: Terraform Certified, practical IaC experience, GitLab/GitHub CI, SaltStack/Ansible background â†’ good for pipeline infra.
  - Kubernetes: KCNA certificate and exposure to EKS, k3s, Minikube and container tooling in trainings/projects â†’ basic/working K8s knowledge.
  - Domain: several projects for logistics and transport clients (Logistik-/Postunternehmen, international logistics/transport) â†’ domain fit.

- Gaps / weak or missing evidence:
  - Big-data tooling (Redis, Apache Flink, Hadoop): CV does not show practical, project-level experience with Redis, Flink or Hadoop (training/service-lists include EMR etc., but no explicit handsâ€‘on projects using Flink/Hadoop/Redis in production).
  - Data validation frameworks: no explicit experience with Great Expectations, Pandera or TFDV is listed.
  - Kubernetes: KCNA and mentions of EKS/k3s show knowledge, but explicit evidence of managing K8s clusters in 3+ distinct client projects is weak/non-explicit.
  - Specific AWS analytics services in real projects (Athena, Lambda usage for data pipelines, CDK): these appear in training/service lists but limited project-level proof for the requested items.
  - Requirement of showing the above in multiple (>=3) projects is not fully met for Big Data / validation frameworks / K8s cluster ops.

- Overall assessment / rationale for score (65/100):
  - The candidate is a strong senior cloud/DevOps/architecture consultant with excellent AWS, Terraform and long-term Python experience and relevant logistics domain knowledge. Those strengths cover many aspects of the role (pipeline infra, architecture, cloud design, advisory).
  - However, the role demands hands-on, repeated project experience with specific big-data stack components (Redis, Flink, Hadoop) and with data-validation frameworks and demonstrable K8s cluster operations across several projects. Those specific, repeatable evidences are missing or not explicit in the CV, which is critical for a Senior Data Engineer focused on streaming/Big Data.
  - Hence a moderate-to-good fit for architecture/DevOps-heavy or AWS-data-architecture tasks (high confidence) but a lower fit for a pure data-engineer role that requires repeated production experience with Flink/Hadoop/Redis and Great Expectations. Score reflects strengths in cloud architecture and Python, offset by missing explicit Big Data/tooling proof.

---
