---
company: Wavestone Germany
reference_id: '2916634'
scraped_date: '2025-09-08T12:00:08.336329'
source_url: https://www.freelancermap.de/projekt/senior-data-engineer-m-w-d-2916634?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 65% fit score'
  state: rejected
  timestamp: '2025-09-08T12:04:58.692807'
title: Senior Data Engineer (m/w/d)
---


# Senior Data Engineer (m/w/d)
**URL:** [https://www.freelancermap.de/projekt/senior-data-engineer-m-w-d-2916634?ref=rss](https://www.freelancermap.de/projekt/senior-data-engineer-m-w-d-2916634?ref=rss)
## Details
- **Start:** 20.09.2025
- **Von:** Wavestone Germany
- **Eingestellt:** 08.09.2025
- **Ansprechpartner:** Christian Bock
- **Projekt-ID:** 2916634
- **Vertragsart:** Freiberuflich
- **Einsatzart:** Auslastung: 95 %

## Schlagworte
Amazon Web Services, Beratung, Python, Redis, Logistikprozesse, Künstliche Intelligenz, Big Data, Geschäftsanforderungen, Cloud Computing, Datenvalidierung, Datenmodell, Datenqualität, Relationale Datenbanken, Apache Hadoop, Multidisziplinären Ansatz, Umstrukturierung, Streaming, Verwaltungstätigkeiten, Datenstrategie, Kubernetes, Apache Flink

## Beschreibung
Wir suchen einen Senior Data Engineer für das Projekt 'KI in der Disposition' zur Weiterentwicklung von Datenpipelines und datengetriebenen Komponenten zur Unterstützung von Dispositionsentscheidungen

Weitere Rahmendaten
Einsatzart:
Auslastung: 95 %
Branche: Transport - Travel - Logistik

Ihre Aufgaben
• Technische Weiterentwicklung unserer Datenpipelines (Batch und Streaming) in der AWS Cloud im Python Code
• Sicherstellung der Datenqualität und Entwurf relationaler und nicht-relationaler Datenmodelle zur Erfüllung der Geschäftsanforderungen
• Entwicklung und Betrieb von internen und externen Schnittstellen zur Bereitstellung von Datenprodukten
• Weiterentwicklung unserer Datenstrategie
• Beratung der Projektmitglieder zum Einsatz unterschiedlicher AWS-Technologien und zur Datenqualitätssicherung
• Entwicklung datengesteuerter Softwarekomponenten zur Optimierung von Dispositionsentscheidungen und verantwortet ein möglichst reibungsloses, effizientes und fehlerfreies Bereitstellen der Datenquellen und deren Aggregationen

Muss-Anforderungen
• Umfangreiche und aktuelle Kenntnisse im Aufbau und in der Restrukturierung von Datenarchitekturen, insbesondere relationaler und nicht-relationaler Datenbanken, nachgewiesen durch Berufserfahrung in mindestens 3 Kundenprojekten
• Praktische Erfahrungen mit Big-Data-Tools (z.B. Redis, Flink, Hadoop), nachgewiesen in mindestens 3 Projekten
• Praktische Erfahrung mit Frameworks zur Datenvalidierung (z.B. Great Expectations, Pandera oder TFDV), mindestens 3 Jahre Berufserfahrung
• Erfahrung in der Verwaltung von Kubernetes-Clustern, nachgewiesen in mindestens 3 Projekten
• Umfangreiche Erfahrung in der Python-Programmierung, mindestens 10 Jahre Berufserfahrung

Kann-Anforderungen
• Erfahrungen in anderen Großkonzernen, nachgewiesen durch Projekte in mindestens einem weiteren Konzern
• Erfahrung im Sektor Transport und Logistik, nachweislich durch mindestens ein Datenprojekt innerhalb des Sektors
• Fließend in Deutsch (C1) mit Nachweis der Qualifikation über Zertifikat/Projekterfahrung
• Erfahrung mit Great Expectations, nachgewiesen durch Projekterfahrung
• Erfahrung mit Lambda, nachgewiesen durch Projekterfahrung
• Erfahrung mit CDK, nachgewiesen durch Projekterfahrung
• Erfahrung mit Athena, nachgewiesen durch Projekterfahrung
• Erfahrung mit Redis, nachgewiesen durch Projekterfahrung

Weitere Informationen
Einsatzort Berlin (onshore); teilweise Verteilung zwischen Remote und Onsite möglich; regelmäßige Onsite-Aktivitäten für Entwurf und gemeinsame Workshops vor Ort vorgesehen; Einsatzzeitraum 20.09.2025 - 31.12.2025; Gesamtaufwand 70 Personentage, davon 7 Onsite-PT

Let’s power the future together
Vom Business Case bis hin zur Umsetzung: Als führendes Beratungsunternehmen für strategische Transformationen sind wir vertrauenswürdiger Partner für unsere Kunden - und für unsere Mitarbeitenden. Verantwortungsvoll, leistungsstark und immer mit dem Menschen im Fokus. #WeAreWavestone
Mit unserem 360°-Portfolio an Beratungsleistungen verbinden wir erstklassige Branchenexpertise mit einem breiten Spektrum an branchenübergreifenden Kompetenzen, arbeiten interdisziplinär und denken über den Tellerrand hinaus. Unseren Partnerunternehmen und Freelancern:Freelancerinnen können wir so umfassende Perspektiven innerhalb unserer eigenen Projekte bieten und unterstützen als langjähriger Rahmenvertragspartner bei der Besetzung von Projektvakanzen – zeitnah und direkt.

Wir freuen uns auf Ihre Kontaktaufnahme!

Ihr direkter Ansprechpartner bei Wavestone
Christian Bock
Telefon:
E-Mail:

---

## 🤖 AI Evaluation Results

**Evaluation Timestamp:** 2025-09-08T12:04:58.688766

### Pre-Evaluation Phase
- **Score:** 19/100
- **Threshold:** 10/100
- **Result:** ✅ Passed
- **Rationale:** Score: 19%. Found tags: ['aws', 'amazon', 'ai', 'ki', 'cloud', 'kubernetes', 'architekt', 'architektur', 'pipeline', 'python', 'rest', 'beratung', 'workshop', 'data', 'big data', 'streaming', 'remote', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 65/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ❌ REJECTED

#### Detailed Rationale
- Key requirements (from project):
  - Strong Python (>=10 years) for batch & streaming pipelines
  - Experience building/restructuring data architectures (relational & non-relational) across >=3 client projects
  - Practical experience with big-data tools (Redis, Flink, Hadoop) in >=3 projects
  - Experience with data-validation frameworks (Great Expectations, Pandera, TFDV) ~3 years
  - Kubernetes cluster management experience proven in >=3 projects
  - AWS experience and ability to advise on AWS technologies (streaming, analytics)

- Strong matches from CV:
  - Python: many projects list Python (automation, integration, CI/CD, tooling) across decades → meets 10+ years requirement.
  - Data architecture / databases: extensive enterprise architecture work (Oracle DB, RDS, migrations, SAP on AWS, ETL/EMR listed in training) and multiple migrations/landing-zone projects → strong experience designing/resctructuring architectures.
  - AWS: multiple professional AWS certs (Solutions Architect Pro/Assoc, SysOps, Developer, AI Practitioner) and numerous AWS services listed and used in projects (Landing Zone, EC2, CloudFormation, etc.) → strong cloud competency.
  - DevOps / IaC: Terraform Certified, practical IaC experience, GitLab/GitHub CI, SaltStack/Ansible background → good for pipeline infra.
  - Kubernetes: KCNA certificate and exposure to EKS, k3s, Minikube and container tooling in trainings/projects → basic/working K8s knowledge.
  - Domain: several projects for logistics and transport clients (Logistik-/Postunternehmen, international logistics/transport) → domain fit.

- Gaps / weak or missing evidence:
  - Big-data tooling (Redis, Apache Flink, Hadoop): CV does not show practical, project-level experience with Redis, Flink or Hadoop (training/service-lists include EMR etc., but no explicit hands‑on projects using Flink/Hadoop/Redis in production).
  - Data validation frameworks: no explicit experience with Great Expectations, Pandera or TFDV is listed.
  - Kubernetes: KCNA and mentions of EKS/k3s show knowledge, but explicit evidence of managing K8s clusters in 3+ distinct client projects is weak/non-explicit.
  - Specific AWS analytics services in real projects (Athena, Lambda usage for data pipelines, CDK): these appear in training/service lists but limited project-level proof for the requested items.
  - Requirement of showing the above in multiple (>=3) projects is not fully met for Big Data / validation frameworks / K8s cluster ops.

- Overall assessment / rationale for score (65/100):
  - The candidate is a strong senior cloud/DevOps/architecture consultant with excellent AWS, Terraform and long-term Python experience and relevant logistics domain knowledge. Those strengths cover many aspects of the role (pipeline infra, architecture, cloud design, advisory).
  - However, the role demands hands-on, repeated project experience with specific big-data stack components (Redis, Flink, Hadoop) and with data-validation frameworks and demonstrable K8s cluster operations across several projects. Those specific, repeatable evidences are missing or not explicit in the CV, which is critical for a Senior Data Engineer focused on streaming/Big Data.
  - Hence a moderate-to-good fit for architecture/DevOps-heavy or AWS-data-architecture tasks (high confidence) but a lower fit for a pure data-engineer role that requires repeated production experience with Flink/Hadoop/Redis and Great Expectations. Score reflects strengths in cloud architecture and Python, offset by missing explicit Big Data/tooling proof.

---
