---
company: Cegeka Deutschland GmbH
reference_id: '2916387'
scraped_date: '2025-09-05T17:00:02.596037'
source_url: https://www.freelancermap.de/projekt/data-platform-engineer-azure-und-amp-databricks-10939?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 45% fit score'
  state: rejected
  timestamp: '2025-09-05T17:02:30.750917'
title: 'Data Platform Engineer* Azure &amp; Databricks #10939'
---


# Data Platform Engineer* Azure &amp; Databricks #10939
**URL:** [https://www.freelancermap.de/projekt/data-platform-engineer-azure-und-amp-databricks-10939?ref=rss](https://www.freelancermap.de/projekt/data-platform-engineer-azure-und-amp-databricks-10939?ref=rss)
## Details
- **Start:** 09.2025
- **Von:** Cegeka Deutschland GmbH
- **Eingestellt:** 05.09.2025
- **Ansprechpartner:** Daniel Kr√§mer
- **Projekt-ID:** 2916387
- **Branche:** IT
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 100
                                                % Remote

## Schlagworte
Microsoft Azure, It-Governance, Databricks, SQL, Third Normal Form, Datenbanken, Data Transformation, Data Mining, Python, Scrum, Service Delivery, Lagerverwaltung, Workflows, Azure Data Factory, Snowflake, Data Lineage

## Beschreibung
Abwechslung und Sicherheit im Klartext hei√üt das: Du profitierst von unserer Marktkompetenz und den stabilen, langj√§hrigen Beziehungen zu unseren Kunden in Deutschland und europaweit. Wir haben nationale und internationale Projekte f√ºr Dich. Nat√ºrlich in renommierten Unternehmen und technologisch anspruchsvollen Branchen. Faire Vertragsbedingungen und eine schnelle und unb√ºrokratische Arbeitsweise verstehen sich bei uns von selbst.

- You facilitate the process of connecting various source systems to the central IT governance tools
- You work together with the IT product owners of the source systems and the team responsible for the IT governance tools to transform the data into the common data model of the company and make it centrally available
- You design and implement interfaces for data extraction, data transformation and data loading to and from the IT governance tools
- You design and implement Medallion-layered pipelines (ingest ? refine ? serve) using Databricks
- You optimize the schema for analytics (star/snowflake) and for operational read patterns
- You implement Unity Catalog, data lineage, access controls, PII handling, and quality checks
- Youll design Medallion (Bronze/Silver/Gold) data pipelines, craft robust database schemas for inventory operations
- You create technical and functional documentation and publish them regularly

- You have proven experience with the Microsoft Azure technology stack, especially Azure Data Factory (ADF) and Azure Databricks
- You have strong dimensional and 3NF modeling knowledge; are comfortable with CDC, late/dirty data, and schema evolution.
- You have proven experience developing with Python and SQL
- You have experience implementing Medallion architecture, Delta Lake, Unity Catalog, Workflows, SQL Warehouse, and Delta Live Tables.
- You show strong analytical and structured thinking abilities
- You show strong problem-solving skills and the ability to work collaboratively in a team environment
- You are fluent in English language

Wir sind ein europ√§ischer IT-Solutions Provider. Unser Ziel: Durch passgenaue IT-Dienstleistungen unterst√ºtzen wir unsere Kunden dabei effizienter zu werden. Wir setzen auf modernste Technologien und ma√ügeschneiderte L√∂sungen. Ganz wichtig: H√∂chste Qualit√§t auf Augenh√∂he wir arbeiten stets partnerschaftlich und respektvoll mit unseren Kunden und im Team. Das Ergebnis wir agieren international, sind aber auf dem Boden geblieben! Als Familienunternehmen mit mehr als 9.000 MitarbeiterInnen in 20 europ√§ischen L√§ndern verbinden wir globale Reichweite mit famili√§ren Werten.

Wenn unserer Stellenausschreibung Dein Interesse weckt, dann melde Dich einfach. Wir freuen uns! Noch ein kurzer Hinweis zum obligatorischen Sternchen (*): Wir suchen M/E/N/S/C/H/E/N Deine St√§rken und Deine Pers√∂nlichkeit z√§hlen. Alles andere ist super, wenn es Dich gl√ºcklich macht. Wir leben Vielfalt und Chancengleichheit schon lange, das ist nichts Neues f√ºr uns.

Come as you are!

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-05T17:02:30.747296

### Pre-Evaluation Phase
- **Score:** 13/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 13%. Found tags: ['ai', 'ki', 'architect', 'architecture', 'pipeline', 'python', 'database', 'sql', 'scrum', 'data', 'analytics', 'remote', 'freelance', 'projekt']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 45/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements from the project:
  - Proven experience with Microsoft Azure stack, specifically Azure Data Factory (ADF) and Azure Databricks
  - Designing and implementing Medallion-layered pipelines on Databricks (Bronze/Silver/Gold), Delta Lake, Delta Live Tables
  - Unity Catalog, data lineage, access controls, PII handling and quality checks
  - Strong Python and SQL development skills
  - Dimensional modeling (star/snowflake) and 3NF modeling experience; comfort with CDC, late/dirty data and schema evolution
  - Experience with Workflows, SQL Warehouse, Delta Live Tables, and production-ready data platform practices
  - Good documentation and collaboration skills; fluent English

- Matches in the CV:
  - Strong cloud and platform experience (extensive AWS background, many services, Landing Zone design, multi-account architectures)
  - Proven experience with data-related projects: migrations, building multi-environment data platforms, SAP BW/4HANA on AWS (data-warehouse design mindset)
  - Solid scripting/dev skills: Python, SQL, bash, Terraform (IaC), CI/CD (GitLab, Jenkins), Docker, Kubernetes knowledge (KCNA)
  - Experience with building repeatable, automated deployments, data migrations and operational runbooks; emphasis on documentation (live-scripting project)
  - Security, access and operational hardening experience (SSL, AD integration, secrets management, Vault exposure)
  - Fluent English and many years of enterprise/consulting experience

- Gaps / risks relative to the role:
  - No explicit Microsoft Azure experience listed (no ADF, Azure Databricks, Azure-specific services called out)
  - No explicit Databricks artifacts: Delta Lake, Delta Live Tables, Unity Catalog, Databricks Workflows or Unity Catalog experience
  - No explicit mention of Medallion pipeline implementation (Bronze/Silver/Gold) on Databricks
  - Dimensional modeling (star/snowflake) and CDC/schema evolution are not clearly demonstrated (strong DB background but not explicit DW modeling on modern lake-house platforms)
  - Snowflake experience listed in project keywords but not shown on CV

- Overall assessment / recommendation:
  - The candidate is a strong, senior cloud/platform engineer with excellent transferable skills (Python/SQL, automation, IaC, data migrations, data-warehouse mindset on AWS). However, the project requires hands-on, specific Azure Databricks and Delta/Unity Catalog experience which is not evident on the CV. With a short ramp-up (or if the client accepts AWS-to-Azure/platform transferability), the candidate could likely deliver. For immediate plug-and-play Databricks/ADF expectations the fit is limited.

- Fit rationale summary: moderate fit due to strong cloud, automation and data migration skills (transferable) but lacking the platform-specific, Databricks/Azure and Delta/Unity Catalog experience required for a high-confidence immediate contribution.

---
