---
company: Hays AG
reference_id: '2915126'
scraped_date: '2025-09-02T22:12:27.300662'
source_url: https://www.freelancermap.at/projekt/data-engineer-m-f-d-2915126?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 50% fit score'
  state: rejected
  timestamp: '2025-09-02T22:14:36.486450'
title: Data Engineer (m/f/d)
---


# Data Engineer (m/f/d)
**URL:** [https://www.freelancermap.at/projekt/data-engineer-m-f-d-2915126?ref=rss](https://www.freelancermap.at/projekt/data-engineer-m-f-d-2915126?ref=rss)
## Details
- **Start:** ab sofort
- **Von:** Hays AG
- **Eingestellt:** 02.09.2025
- **Ansprechpartner:** Hays AG
- **Projekt-ID:** 2915126
- **Branche:** IT
- **Vertragsart:** Freiberuflich

## Schlagworte
Databricks, Pyspark, Python, Apache Spark, Objektorientierte Software-Entwicklung, Architektur, Continuous Integration, Performance-Tuning, Software Architecture

## Beschreibung
**Data Engineer (m/f/d)**

Reference: -en
Start: asap
Duration: 6 MM++

**Main tasks:**

- Develop and enhance a streaming pipeline using Databricks DLT and PySpark
- Optimize PySpark code for efficient processing of streaming data
- Maintain and deploy a Databricks Asset Bundle (DAB) for structured management of project artifacts
- Further develop and implement object-oriented Python components, especially in the context of Apache Spark
- Performance tuning and code refactoring to ensure scalability and maintainability

**Main qualifications:**

- Solid experience developing with PySpark and Apache Spark
- Deep understanding of streaming architectures and related challenges
- Hands-on experience with Databricks DLT and DAB
- Proficiency in object-oriented programming in Python
- Familiarity with deployment processes in Databricks environments
- TechStack: Databricks, Apache Spark, PySpark, Python, CI/CD f√ºr Databricks Deployment

**Main advantages:**

- International client
- Remote-option

**About us:**
IT is and always has been our core business that laid the foundation for Hays' success. We are the biggest privately owned IT recruitment agency in Germany and offer the best jobs for every career level ‚Äì whether you are interested in vacancies in agile SMEs or international DAX groups. Hays masters the entire IT job spectrum, from support to software architecture or digitalisation ‚Äì thanks to our broad portfolio, we have something for everyone. In the last decades, we were able to support numerous IT experts with choosing the right path for a successful career, positioning ourselves as their lifelong partner. Our highly specialised consultants can cater to your every wish and expectation and will prepare you for interviews and contract negotiations. Give it a try and learn what the market has to offer ‚Äì our services are free of charge, non-binding and discreet! We look forward to hearing from you.

**My contact at Hays:**

My contact person:
Vincent Ru√ü

Referencenumber:

Make contact:
Email:

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-02T22:14:36.484218

### Pre-Evaluation Phase
- **Score:** 20/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 20%. Found tags: ['ai', 'architect', 'architekt', 'architecture', 'architektur', 'ci/cd', 'pipeline', 'python', 'git', 'rest', 'integration', 'agile', 'continuous', 'deployment', 'consultant', 'data', 'streaming', 'scalability', 'performance', 'remote', 'freelance', 'projekt', 'project', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 50/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements from the project:
  - Databricks (DLT), Databricks Asset Bundle (DAB) and Databricks deployment pipelines
  - PySpark / Apache Spark (streaming + batch) and optimization of PySpark code
  - Strong Python OOP skills and code refactoring / performance tuning for streaming architectures
  - CI/CD for Databricks deployments and operational knowledge of Databricks environments
  - Experience with streaming architectures and scalability concerns

- Matches / Strengths in the CV:
  - Strong cloud architecture background (many AWS certifications incl. Solutions Architect Pro) and broad knowledge of AWS data services (EMR, Kinesis, Glue, etc.) ‚Äî relevant for cloud-based datapipelines
  - Substantial DevOps / IaC / automation experience: Terraform, CI/CD (GitLab CI/CD, experience with deployment automation), Docker, Kubernetes ‚Äî useful for building reproducible Databricks deployments
  - Solid Python experience (used across many projects) and long track record of performance tuning, architecture and high-availability systems
  - Experience with streaming/messaging and enterprise integration (OSB, JMS, ESB), which demonstrates understanding of streaming architecture challenges (latency, ordering, fault tolerance)
  - Demonstrated capability to learn and adopt new technologies (recent upskilling: Terraform, KCNA, GitHub Copilot, AWS AI certs) ‚Äî indicates ramp-up ability

- Gaps / Concerns vs. explicit project needs:
  - No explicit hands-on experience with Databricks platform, Databricks DLT (Delta Live Tables) or Databricks Asset Bundle (DAB) shown in projects or technologies
  - No clear evidence of PySpark/Apache Spark production work or streaming with Spark Structured Streaming; most streaming/integration experience is ESB/JMS/Oracle-focused
  - No explicit examples of CI/CD for Databricks deployments (DAB-specific pipelines) or Databricks workspace automation
  - The CV is strong on architecture and operations but many recent projects focus on Oracle/OSB/Weblogic migrations rather than data-engineering on Databricks/Spark

- Recommendation / ramp-up assessment:
  - Candidate appears able to bridge gaps quickly given strong cloud, Python and DevOps background. Expect a short ramp-up (~4‚Äì8 weeks) to become fully productive on Databricks/DLT/DAB if given access to a sandbox and initial pairing with an experienced Databricks engineer.

- Score breakdown (qualitative weights):
  - Core platform & tooling (Databricks, DLT, DAB): -40 points (missing)
  - PySpark / Spark streaming production experience: -20 points (missing)
  - Python OOP, performance tuning, streaming architecture knowledge: +20 points (good match)
  - Cloud, DevOps, CI/CD, IaC experience (Terraform, CI pipelines, deployment automation): +25 points (strong match)
  - Learning ability / certifications and recent upskilling: +15 points

- Overall assessment: 50/100 ‚Äî moderate fit. Strong cloud/DevOps and Python/architecture skills make the candidate a reasonable hire for the role if the client accepts a short onboarding period for platform-specific Databricks/DLT/PySpark ramp-up.

---
