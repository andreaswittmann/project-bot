---
company: Venturi Germany GmbH
reference_id: '2915887'
scraped_date: '2025-09-04T16:00:09.477761'
source_url: https://www.freelancermap.de/projekt/gesucht-sap-abap-abap-oo-developer-m-w-d-2915887?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 65% fit score'
  state: rejected
  timestamp: '2025-09-04T16:05:12.088196'
title: 'Gesucht: Data Engineer im Bereich Specialty & Global Data Warehouse (m, w,
  d)'
---


# Gesucht: Data Engineer im Bereich Specialty & Global Data Warehouse (m, w, d)
**URL:** [https://www.freelancermap.de/projekt/gesucht-sap-abap-abap-oo-developer-m-w-d-2915887?ref=rss](https://www.freelancermap.de/projekt/gesucht-sap-abap-abap-oo-developer-m-w-d-2915887?ref=rss)
## Details
- **Start:** 01.2026
- **Von:** Venturi Germany GmbH
- **Auslastung:** 80% (4 Tage pro Woche)
- **Eingestellt:** 04.09.2025
- **Ansprechpartner:** Ramith Rayananthan
- **Projekt-ID:** 2915887
- **Branche:** IT
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 80
                                                % Remote

## Schlagworte
Data Warehousing, Airflow, ETL, SQL, Snowflake, ABAP, Data Analysis, Architektur, Versicherungen, Qualit√§tsmanagement, SAP ABAP, Stakeholderkommunikation, Gitlab, Software Version Control

## Beschreibung
Guten Tag,

F√ºr ein modernes, cloudbasiertes Data-Warehouse-Umfeld wird ein erfahrener Data Engineer gesucht. Im Mittelpunkt stehen die Weiterentwicklung der bestehenden Architektur sowie die Harmonisierung von Daten f√ºr √ºbergreifende Auswertungen.

Allgemeine Informationen:
- Zeitraum des Einsatzes: 01.01.2026 - 31.12.2026
- Auslastung: ca. 180 Tage => ca. 3,5 Tage / Woche
- Haupts√§chlich Remote. Ca. 25 Tage On-Site

Aufgaben
- Weiterentwicklung und Erweiterung der Data-Warehouse-Strukturen
- Modellierung von DataVault- und Data-Mart-Strukturen
- Entwicklung, Anpassung und √úberwachung von ETL-Prozessen (z. B. dbt, Airflow, Snowflake)
- Analyse und Behebung von Fehlern in Datenverarbeitungsprozessen
- Durchf√ºhrung von Datenanalysen (SQL) zur Unterst√ºtzung von Fachanforderungen
- Abstimmung und Kommunikation mit Stakeholdern
- Unterst√ºtzung bei Test- und Qualit√§tssicherungsma√ünahmen

Anforderungen
- Fachlich:
-> Sehr gute SQL-Kenntnisse
-> Erfahrung in Data-Warehouse-Projekten
-> Praktische Kenntnisse g√§ngiger ETL-Tools (z. B. dbt, Airflow, Snowflake) und Versionskontrolle (GitLab)
-> Erfahrung in DataVault- und/oder dimensionaler Modellierung
-> Branchenkenntnisse (z. B. Versicherung) von Vorteil

Pers√∂nlich:
-> Starke Kommunikations- und Moderationsf√§higkeiten
-> Sehr gute Ausdrucksweise in Deutsch und Englisch (Wort & Schrift)

Wenn Sie Interesse an diesem Projekt haben, senden Sie mir bitte Ihren aktuellen Lebenslauf und die Folgenden Informationen zu: Verf√ºgbarkeit + Auslastung? Remotesatz?

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-04T16:05:12.084451

### Pre-Evaluation Phase
- **Score:** 10/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 10%. Found tags: ['ai', 'cloud', 'architekt', 'architektur', 'gitlab', 'git', 'sql', 'data', 'etl', 'remote', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 65/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements (from project): very strong SQL; experience in Data-Warehouse projects; ETL tooling (dbt, Airflow, Snowflake) and CI/VCS (GitLab); DataVault or dimensional modelling; stakeholder communication; insurance domain a plus.
- Strong matches from CV:
  - Extensive data and DW architecture experience (SAP BW/4HANA on AWS, multiple DW-related projects).
  - Deep AWS/cloud background and hands-on with many data-related AWS services (Glue, Athena, EMR, Kinesis, Redshift listed in trainings), which maps well to cloud-based DW thinking.
  - Very good SQL experience (Oracle DB, many migrations and analyses) and scripting (Python, bash).
  - Experience with CI/CD and version control (Git, GitLab CI/CD in projects).
  - Solid stakeholder communication and consultancy experience across many enterprise clients; German & English fluency.
- Gaps / weak or missing evidence:
  - No explicit hands-on experience with Snowflake, dbt or Airflow mentioned (project requires practical knowledge of these specific tools).
  - No explicit DataVault modelling experience (only SAP BW/4HANA and dimensional/DW design references).
  - No explicit ABAP coding experience (job posting tags ABAP, though the role description is focused on Data Engineering).
  - Limited explicit evidence of recent, high-volume pipeline development/monitoring with the exact stack requested (e.g., Snowflake+dbt+Airflow).
- Overall assessment: Candidate is a strong enterprise/architect-level DW engineer with excellent cloud, SQL and AWS experience and good domain/consulting skills. The main shortfall vs this brief is lack of explicit Snowflake/dbt/Airflow and DataVault hands-on evidence. For a client willing to accept migration of skills (AWS/data services ‚Üí Snowflake/dbt/Airflow) and focus on architecture/mentoring rather than pure implementation, fit is good; for strict requirement of immediate, hands‚Äëon dbt/Airflow/Snowflake specialists the fit is moderate.
- Recommendation: Consider for architecture/lead/mentoring role on the engagement or as a Data Engineer if client is flexible and values AWS + SAP BW experience; clarify hands‚Äëon availability to ramp up Snowflake/dbt/Airflow quickly.

---
