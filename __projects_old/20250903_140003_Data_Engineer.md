---
company: skillbyte GmbH
reference_id: '2915341'
scraped_date: '2025-09-03T14:00:03.718839'
source_url: https://www.freelancermap.de/projekt/data-engineer-2915341?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 55% fit score'
  state: rejected
  timestamp: '2025-09-03T14:02:04.099029'
title: Data Engineer
---


# Data Engineer
**URL:** [https://www.freelancermap.de/projekt/data-engineer-2915341?ref=rss](https://www.freelancermap.de/projekt/data-engineer-2915341?ref=rss)
## Details
- **Start:** ab sofort
- **Von:** skillbyte GmbH
- **Auslastung:** 100% (5 Tage pro Woche)
- **Eingestellt:** 03.09.2025
- **Ansprechpartner:** Jana K√§sbach
- **Projekt-ID:** 2915341
- **Branche:** IT
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 100
                                                % Remote

## Schlagworte
Microsoft Azure, Cloud Computing, Big Data, Amazon Web Services, Continuous Integration, Devops, Python, Scala, SQL, Apache Spark, Kubernetes, K√ºnstliche Intelligenz, Airflow, Confluence, Jira, Bash Shell, Bigquery, Datenbanken, ETL, Data Migration, Github, Apache Hadoop, Apache Hive, Postgresql, Oracle Financials, Scrum, Informix, Google Cloud, Grafana, Fastapi, Gitlab-Ci, Apache Flink, Apache Kafka, Cosmos DB, Machine Learning Operations, Terraform, Docker, Jenkins, Databricks

## Beschreibung
Beschreibung:
F√ºr unser Daten-Plattform- und KI-Projekt suchen wir aktuell eine*n Data Engineer (m/w/d).
Die Hauptaufgabe besteht darin, die Konzeption, Entwicklung und Optimierung von ETL-Pipelines und Cloud-Infrastrukturen bei der Konzeption, Umsetzung und Optimierung der Big-Data-Plattformen auf Basis von Cloud-Technologien (AWS, Azure, GCP) zu unterst√ºtzen.

Aufgabe:
- Umsetzung und Optimierung der Big-Data-Pipelines
- Integration und Nutzung von CI/CD-Pipelines
- Unterst√ºtzung bei Datenmigrationen

Anforderung:
- Sehr gute Kenntnisse in Python, Scala, SQL, Spark und Kubernetes
- 7 Jahre lange Erfahrung in der Cloud-Entwicklung
- Gute Kenntnisse in DevOps und MLOps

Technologie:
- Cloud: AWS, Azure, Google Cloud
- Big Data: Spark, Hadoop, Databricks, Apache Airflow, Kafka, Hive, Flink
- Datenbanken: PostgreSQL, Oracle, Informix, BigQuery, Azure Cosmos DB
- CI/CD & DevOps: GitLab CI/CD, GitHub Actions, Azure DevOps Pipelines, Jenkins, Docker, Kubernetes, Terraform, GitOps
- Sprachen: Python, Scala, SQL, Bash
- Sonstige: Grafana, Jira, Scrum, Confluence, FastAPI

Rahmendaten
- Start: ASAP
- Laufzeit: Ende Dezember + Option auf Verl√§ngerung
- Auslastung: Vollzeit, 40 Stunden
- Ort: Remote
- Sprachen: Deutsch, Englisch

Wir haben Dein Interesse geweckt?
Dann sende uns Deinen CV und Deinen Stundensatz zu. Gerne machen wir dann bei √úbereinstimmung ein pers√∂nliches Gespr√§ch aus.

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-03T14:02:04.096458

### Pre-Evaluation Phase
- **Score:** 30/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 30%. Found tags: ['aws', 'amazon', 'ai', 'ki', 'machine learning', 'ml', 'cloud', 'terraform', 'kubernetes', 'docker', 'grafana', 'devops', 'ci/cd', 'pipeline', 'jenkins', 'gitlab', 'python', 'bash', 'shell', 'git', 'github', 'api', 'oracle', 'sql', 'integration', 'scrum', 'continuous', 'data', 'big data', 'etl', 'remote', 'freelance', 'projekt', 'jira', 'confluence']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 55/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key job requirements: very good knowledge of Python, Scala, SQL, Spark and Kubernetes; 7+ years cloud development (AWS/Azure/GCP); Big Data stack (Spark, Hadoop, Databricks, Airflow, Kafka, Hive, Flink); DBs (Postgres, Oracle, Informix, BigQuery, CosmosDB); CI/CD & DevOps (GitLab CI, GitHub Actions, Azure DevOps, Jenkins, Docker, Kubernetes, Terraform); familiarity with MLOps.

- Strong matches from CV:
  - Cloud: deep AWS experience (multiple migrations, Landing Zone, EKS references) and recent AWS certifications (Solutions Architect Pro, AI Practitioner). Meets the 7+ years cloud requirement.
  - Kubernetes: certified (KCNA) and practical work with EKS, k3s, Minikube.
  - Python and Bash: multiple projects list Python and shell-scripting for automation and migration tasks.
  - DevOps / CI-CD / IaC: hands-on with Jenkins, GitLab CI/CD, Git, Docker, Terraform (HashiCorp Certified), Salt/Ansible exposure and Infrastructure-as-Code practice.
  - Databases: extensive Oracle experience (many projects); general SQL and migration experience.
  - MLOps / AI: recent projects using AWS Bedrock, SageMaker awareness, private AI (LibreChat) ‚Äî shows familiarity with AI stack and responsible AI concepts.
  - Observability: basic/working knowledge of Prometheus and Grafana.

- Partial / acceptable matches:
  - Multi-cloud: mostly AWS-centric; some multi-cloud statements in LibreChat project but limited explicit GCP/Azure hands‚Äëon experience.
  - CI tooling: GitLab CI and Jenkins present; GitHub/GitHub Actions known (Copilot cert), Azure DevOps not explicitly shown but transferable skills likely.

- Gaps versus the offer (important):
  - Core Big Data technologies: no explicit hands‚Äëon Spark, Scala, Hadoop, Databricks, Airflow, Hive or Flink shown in CV ‚Äî the job explicitly requires very good knowledge of Spark and Scala.
  - Kafka/streaming platforms: not clearly present (mostly JMS/IBM MQ and messaging experience, but not Kafka).
  - Some DBs in spec (PostgreSQL, Informix, BigQuery, Cosmos DB) are not evidenced; Oracle is strong but Postgres/BigQuery/CosmosDB are missing.
  - FastAPI and Scala: not listed in CV.

- Assessment / recommendation: Andreas is a strong cloud architect/DevOps/Data‚Äëplatform engineer with excellent AWS, Terraform, Kubernetes and Python skills and solid experience in migrations and automation. However, the role's explicit demand for very good Spark + Scala and broad Big Data tech (Airflow, Databricks, Kafka, Hive, Flink) is not supported by his CV. He would be a strong candidate for platform/infra or cloud-native data-platform architecture work and could learn/cover Spark/Scala and some Big Data tools quickly given his background, but for an immediate plug-and-play Data Engineer role focused on Spark/Scala-based pipelines he is only a partial fit.

- Fit score rationale: Given strong matches in cloud, Kubernetes, Python, Terraform and CI/CD (positive weight) but important missing core Big Data and Scala/Spark skills (heavy negative weight), final fit score is 55/100.

---


---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-03T14:02:42.396390

### Pre-Evaluation Phase
- **Score:** 46/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 46%. Found tags: ['aws', 'amazon', 'bedrock', 'ai', 'ki', 'llm', 'openai', 'machine learning', 'ml', 'cloud', 'terraform', 'kubernetes', 'docker', 'prometheus', 'grafana', 'architect', 'cloud architect', 'architecture', 'devops', 'ci/cd', 'pipeline', 'jenkins', 'gitlab', 'automation', 'iac', 'ansible', 'python', 'bash', 'shell', 'scripting', 'git', 'github', 'copilot', 'api', 'oracle', 'database', 'sql', 'integration', 'scrum', 'continuous', 'data', 'big data', 'etl', 'streaming', 'remote', 'freelance', 'projekt', 'project', 'jira', 'confluence', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 55/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Key requirements: very good Python, Scala, SQL, Spark, Kubernetes; 7+ years cloud development (AWS/Azure/GCP); Big Data stack (Spark, Hadoop, Databricks, Airflow, Kafka, Hive, Flink); DBs (Postgres, Oracle, Informix, BigQuery, Cosmos DB); CI/CD & DevOps (GitLab CI/CD, GitHub Actions, Azure DevOps, Jenkins, Docker, Kubernetes, Terraform); MLOps knowledge.
- Strong matches: deep AWS experience (multiple projects + Solutions Architect Pro), Terraform (certified), Kubernetes (KCNA, EKS), Python and Bash scripting, CI/CD and DevOps (GitLab CI, Jenkins, Git, Docker), Infrastructure-as-Code and migration experience, extensive Oracle/SQL background, demonstrable MLOps/AI exposure (AWS Bedrock, SageMaker awareness, private AI projects), basic observability tools (Grafana/Prometheus exposure).
- Partial / transferable matches: multi‚Äëcloud is claimed but primarily AWS-focused (some multi-cloud in LibreChat); GitHub/GitHub Copilot familiarity; ability to learn new tools quickly given broad infra/automation background.
- Key gaps / risks: no explicit hands‚Äëon Spark or Scala (role requires very good knowledge); no clear evidence of Hadoop, Databricks, Airflow, Hive or Flink; Kafka/modern streaming platforms not shown (mostly JMS/MQ experience); Postgres, BigQuery, CosmosDB, Informix not evidenced; FastAPI and Scala absent from CV.
- Impact: Candidate is a strong cloud/DevOps/data‚Äëplatform architect and would add value for infra, migrations and platform automation. However, the vacancy is Spark/Scala‚Äëcentric with a broad Big Data toolset and streaming requirements ‚Äî those core skills are missing for an immediate plug‚Äëand‚Äëplay Data Engineer.
- Recommendation & score rationale: Positive weighting for cloud, Kubernetes, Terraform, Python and CI/CD; heavy negative weighting for missing Spark/Scala and Big Data technologies. Final fit_score: 55/100.

---
