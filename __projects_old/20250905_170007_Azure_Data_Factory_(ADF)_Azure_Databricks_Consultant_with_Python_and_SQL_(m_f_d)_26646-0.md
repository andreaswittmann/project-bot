---
company: C4 Energy GmbH & Co. KG
reference_id: '2916372'
scraped_date: '2025-09-05T17:00:07.793571'
source_url: https://www.freelancermap.de/projekt/azure-data-factory-adf-azure-databricks-consultant-with-python-and-sql-m-f-d-26646-0?ref=rss
state: rejected
state_history:
- note: 'LLM evaluation: 40% fit score'
  state: rejected
  timestamp: '2025-09-05T17:05:15.218342'
title: Azure Data Factory (ADF) / Azure Databricks Consultant with Python and SQL
  (m/f/d) 26646-0
---


# Azure Data Factory (ADF) / Azure Databricks Consultant with Python and SQL (m/f/d) 26646-0
**URL:** [https://www.freelancermap.de/projekt/azure-data-factory-adf-azure-databricks-consultant-with-python-and-sql-m-f-d-26646-0?ref=rss](https://www.freelancermap.de/projekt/azure-data-factory-adf-azure-databricks-consultant-with-python-and-sql-m-f-d-26646-0?ref=rss)
## Details
- **Start:** 09.2025
- **Von:** C4 Energy GmbH & Co. KG
- **Eingestellt:** 05.09.2025
- **Ansprechpartner:** Sebastian Mende
- **Projekt-ID:** 2916372
- **Branche:** Energiewirtschaft
- **Vertragsart:** Freiberuflich
- **Einsatzart:** 100
                                                % Remote

## Schlagworte
Databricks, Microsoft Azure, SQL, It-Governance, Python, Azure Data Factory, Datenbanken, Relationale Datenbanken, DSGVO, Third Normal Form, Architektur, Anlagenverwaltung, Data Transformation, Data Mining, Machine Learning, Sql Azure, Scrum, Lagerverwaltung, Workflows, Snowflake, Data Lineage

## Beschreibung
Azure Data Factory (ADF) / Azure Databricks Consultant with Python and SQL (m/f/d)

Start-/End-Datum: 01.09.2025 - 31.01.2026
Einsatzort: Hannover
Offsite Stunden: 600

Projektbeschreibung:
We are in the process of modernizing our internally developed Asset Management Tool, which is currently backed by a traditional RDBMS database system. The tool consolidates data from multiple heterogeneous sources to manage financial or operational assets, and includes modules for tracking, reporting, and analysis.

At present, all the data ingestion, processing, and storage are managed directly within a relational database Azure sql server.
Goal:

As we scale, we have identified the need to transition this architecture to a modern, scalable, and maintainable data platform using Databricks.
Our chosen architectural approach is the Medallion Architecture (Bronze, Silver, Gold layers), which will help us standardize data ingestion, transformation, and consumption while supporting future analytical and machine learning use cases.
Also connecting the Power BI reports directly with this.

Tasks
‚Ä¢ You facilitate the process of connecting various source systems to the central IT governance tools
‚Ä¢ You work together with the IT product owners of the source systems and the team responsible for the IT governance tools to transform the data into the common data model and make it centrally available
‚Ä¢ You design and implement interfaces for data extraction, data transformation and data loading to and from the IT governance tools
‚Ä¢ You design and implement Medallion-layered pipelines (ingest ? refine ? serve) using Databricks
‚Ä¢ You optimize the schema for analytics (star/snowflake) and for operational read patterns
‚Ä¢ You implement Unity Catalog, data lineage, access controls, PII handling, and quality checks
‚Ä¢ You‚Äôll design Medallion (Bronze/Silver/Gold) data pipelines, craft robust database schemas for inventory operations
‚Ä¢ You create technical and functional documentation and publish them regularly

SKILLS
Skills/profile
‚Ä¢ You have proven experience with the Microsoft Azure technology stack, especially Azure Data Factory (ADF) and Azure Databricks ‚Ä¢ You have strong dimensional and 3NF modeling knowledge; are comfortable with CDC, late/dirty data, and schema evolution.
‚Ä¢ You have proven experience developing with Python and SQL ‚Ä¢ You have experience implementing Medallion architecture, Delta Lake, Unity Catalog, Workflows, SQL Warehouse, and Delta Live Tables.
‚Ä¢ You show strong analytical and structured thinking abilities ‚Ä¢ You show strong problem-solving skills and the ability to work collaboratively in a team environment ‚Ä¢ You are fluent in English language

Hinweis:
Ab Mai 2018 gilt die neue EU-DSGVO, damit wir Ihre Bewerbung ber√ºcksichtigen und Ihnen zuk√ºnftig weiterhin direkt Projekte aktiv vorstellen k√∂nnen, best√§tigen Sie uns bitte einmal in Ihrer Bewerbung, dass wir Ihr Profil bei uns speichern d√ºrfen.

Als Zustimmung bitte einfach copy&paste EU-DSGVO OK! in Ihre Bewerbung kopieren.

Mit dem √úbersenden Ihres CVs im Rahmen Ihrer Bewerbung auf diesem Portal und/oder per Email erkl√§ren Sie sich damit einverstanden, dass wir Ihr Profil mit Ihren pers√∂nlichen Daten f√ºr unsere weitere Zusammenarbeit bei uns aufnehmen. Eine Weitergabe Ihrer Daten und Vorstellung bei unseren Kunden erfolgt selbstverst√§ndlich nur mit Ihrem Einverst√§ndnis und nach R√ºcksprache mit Ihnen.
Sollten Sie keine weiteren E-Mails und/oder die L√∂schung Ihrer Daten von uns w√ºnschen, schreiben Sie eine E-Mail an mit dem Betreff "Abmelden". Wir respektieren Ihr Recht auf Privatsphaere. Lesen Sie hierzu unsere Richtlinien unter c4-energy.com/Datenschutz.html. Unsere Stelle f√ºr Datenschutz erreichen Sie unter

---

## ü§ñ AI Evaluation Results

**Evaluation Timestamp:** 2025-09-05T17:05:15.214513

### Pre-Evaluation Phase
- **Score:** 19/100
- **Threshold:** 10/100
- **Result:** ‚úÖ Passed
- **Rationale:** Score: 19%. Found tags: ['ai', 'ki', 'machine learning', 'ml', 'architect', 'architekt', 'architecture', 'architektur', 'pipeline', 'python', 'database', 'sql', 'scrum', 'consultant', 'data', 'analytics', 'remote', 'freelance', 'projekt', 'ant']

### LLM Analysis Phase
- **LLM Provider:** OpenAI
- **LLM Model:** gpt-5-mini
- **Fit Score:** 40/100
- **Acceptance Threshold:** 85/100
- **Final Decision:** ‚ùå REJECTED

#### Detailed Rationale
- Matches:
  - ~30 years of IT experience with strong architecture, migration and integration background; proven delivery of complex platform migrations and high-availability systems.
  - Solid hands-on experience with Python and bash scripting, and frequent work with relational databases (Oracle, RDS) and SQL-based migrations.
  - Practical experience designing ETL/migration processes, CI/CD pipelines (GitLab CI, Jenkins), Infrastructure-as-Code (Terraform) and observability basics (Prometheus/Grafana exposure).
  - Experience with cloud architectures and multi-account/landing zone patterns (strong AWS background) which transfers conceptually to Azure designs.

- Gaps / Risks:
  - No explicit Microsoft Azure experience listed (no Azure Data Factory, Azure Databricks, Azure SQL, Synapse, or Azure-specific IAM/Control tooling).
  - No direct Databricks/Delta Lake experience: Unity Catalog, Delta Live Tables, Delta Lake, Databricks Workflows and Databricks-specific optimizations are not shown.
  - Dimensional modelling / analytics schema work (star/snowflake, CDC, schema evolution) not clearly evidenced‚ÄîCV focuses more on middleware, application/platform architecture and Oracle/BEA ecosystems.
  - No explicit Power BI, Unity Catalog/data lineage tooling, or documented PII handling / GDPR implementation in a Databricks/Delta context.

- Conclusion / Recommendation:
  - The candidate is strong on architecture, migrations, Python/SQL and IaC, and could contribute to design, pipeline concepts and migration strategy.
  - However, for a short-term contract that requires immediate Azure Databricks + Delta Lake expertise, the candidate would need ramp-up or pairing with a Databricks/Azure specialist. This reduces immediate fit for the role as advertised.

---
